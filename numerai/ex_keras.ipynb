{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras基本例からNumerai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用データ\n",
    "\n",
    "同じフォルダにnumeraiのtrainingデータセットおくこと."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.592482</td>\n",
       "      <td>0.406464</td>\n",
       "      <td>0.130613</td>\n",
       "      <td>0.585993</td>\n",
       "      <td>0.203948</td>\n",
       "      <td>0.990577</td>\n",
       "      <td>0.065272</td>\n",
       "      <td>0.864658</td>\n",
       "      <td>0.850340</td>\n",
       "      <td>0.116827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976095</td>\n",
       "      <td>0.523960</td>\n",
       "      <td>0.148403</td>\n",
       "      <td>0.600202</td>\n",
       "      <td>0.688142</td>\n",
       "      <td>0.470819</td>\n",
       "      <td>0.286410</td>\n",
       "      <td>0.796651</td>\n",
       "      <td>0.641891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097404</td>\n",
       "      <td>0.135520</td>\n",
       "      <td>0.120604</td>\n",
       "      <td>0.077005</td>\n",
       "      <td>0.564318</td>\n",
       "      <td>0.066696</td>\n",
       "      <td>0.675741</td>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.927066</td>\n",
       "      <td>0.091658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081815</td>\n",
       "      <td>0.128766</td>\n",
       "      <td>0.076859</td>\n",
       "      <td>0.839097</td>\n",
       "      <td>0.576018</td>\n",
       "      <td>0.759510</td>\n",
       "      <td>0.745820</td>\n",
       "      <td>0.303073</td>\n",
       "      <td>0.878545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.910402</td>\n",
       "      <td>0.941407</td>\n",
       "      <td>0.209146</td>\n",
       "      <td>0.912570</td>\n",
       "      <td>0.785759</td>\n",
       "      <td>0.877535</td>\n",
       "      <td>0.762701</td>\n",
       "      <td>0.643769</td>\n",
       "      <td>0.108549</td>\n",
       "      <td>0.293277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943610</td>\n",
       "      <td>0.822201</td>\n",
       "      <td>0.333061</td>\n",
       "      <td>0.782492</td>\n",
       "      <td>0.666455</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>0.608245</td>\n",
       "      <td>0.652344</td>\n",
       "      <td>0.033444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.938794</td>\n",
       "      <td>0.980285</td>\n",
       "      <td>0.452220</td>\n",
       "      <td>0.896527</td>\n",
       "      <td>0.909025</td>\n",
       "      <td>0.391896</td>\n",
       "      <td>0.741063</td>\n",
       "      <td>0.801898</td>\n",
       "      <td>0.133652</td>\n",
       "      <td>0.573551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930512</td>\n",
       "      <td>0.853767</td>\n",
       "      <td>0.317888</td>\n",
       "      <td>0.460311</td>\n",
       "      <td>0.063970</td>\n",
       "      <td>0.434131</td>\n",
       "      <td>0.202171</td>\n",
       "      <td>0.906957</td>\n",
       "      <td>0.297918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.240828</td>\n",
       "      <td>0.754414</td>\n",
       "      <td>0.888972</td>\n",
       "      <td>0.228895</td>\n",
       "      <td>0.172946</td>\n",
       "      <td>0.593818</td>\n",
       "      <td>0.180354</td>\n",
       "      <td>0.623804</td>\n",
       "      <td>0.348972</td>\n",
       "      <td>0.715115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671845</td>\n",
       "      <td>0.324413</td>\n",
       "      <td>0.856480</td>\n",
       "      <td>0.276030</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.333042</td>\n",
       "      <td>0.517633</td>\n",
       "      <td>0.269451</td>\n",
       "      <td>0.320914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.250720</td>\n",
       "      <td>0.402564</td>\n",
       "      <td>0.742476</td>\n",
       "      <td>0.332391</td>\n",
       "      <td>0.586591</td>\n",
       "      <td>0.719378</td>\n",
       "      <td>0.612262</td>\n",
       "      <td>0.709459</td>\n",
       "      <td>0.487905</td>\n",
       "      <td>0.938784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347437</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>0.746829</td>\n",
       "      <td>0.393725</td>\n",
       "      <td>0.658273</td>\n",
       "      <td>0.969089</td>\n",
       "      <td>0.552002</td>\n",
       "      <td>0.707005</td>\n",
       "      <td>0.762388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.301617</td>\n",
       "      <td>0.020043</td>\n",
       "      <td>0.918485</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.976656</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>0.994657</td>\n",
       "      <td>0.112611</td>\n",
       "      <td>0.691903</td>\n",
       "      <td>0.823239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.296781</td>\n",
       "      <td>0.959141</td>\n",
       "      <td>0.976805</td>\n",
       "      <td>0.592507</td>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.538080</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.611522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.418199</td>\n",
       "      <td>0.767178</td>\n",
       "      <td>0.065719</td>\n",
       "      <td>0.423229</td>\n",
       "      <td>0.624257</td>\n",
       "      <td>0.799378</td>\n",
       "      <td>0.572715</td>\n",
       "      <td>0.767760</td>\n",
       "      <td>0.619077</td>\n",
       "      <td>0.191164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685799</td>\n",
       "      <td>0.517264</td>\n",
       "      <td>0.178209</td>\n",
       "      <td>0.595161</td>\n",
       "      <td>0.841767</td>\n",
       "      <td>0.486965</td>\n",
       "      <td>0.813587</td>\n",
       "      <td>0.658656</td>\n",
       "      <td>0.646261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.303557</td>\n",
       "      <td>0.328762</td>\n",
       "      <td>0.829169</td>\n",
       "      <td>0.212381</td>\n",
       "      <td>0.988171</td>\n",
       "      <td>0.146131</td>\n",
       "      <td>0.885027</td>\n",
       "      <td>0.185728</td>\n",
       "      <td>0.188507</td>\n",
       "      <td>0.981038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384778</td>\n",
       "      <td>0.327223</td>\n",
       "      <td>0.983245</td>\n",
       "      <td>0.924070</td>\n",
       "      <td>0.080164</td>\n",
       "      <td>0.333042</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.206736</td>\n",
       "      <td>0.259746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.299323</td>\n",
       "      <td>0.634118</td>\n",
       "      <td>0.094413</td>\n",
       "      <td>0.488458</td>\n",
       "      <td>0.636429</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0.248635</td>\n",
       "      <td>0.981777</td>\n",
       "      <td>0.872428</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569149</td>\n",
       "      <td>0.438558</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>0.282386</td>\n",
       "      <td>0.395210</td>\n",
       "      <td>0.723274</td>\n",
       "      <td>0.222967</td>\n",
       "      <td>0.853229</td>\n",
       "      <td>0.782230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.592482  0.406464  0.130613  0.585993  0.203948  0.990577  0.065272   \n",
       "1  0.097404  0.135520  0.120604  0.077005  0.564318  0.066696  0.675741   \n",
       "2  0.910402  0.941407  0.209146  0.912570  0.785759  0.877535  0.762701   \n",
       "3  0.938794  0.980285  0.452220  0.896527  0.909025  0.391896  0.741063   \n",
       "4  0.240828  0.754414  0.888972  0.228895  0.172946  0.593818  0.180354   \n",
       "5  0.250720  0.402564  0.742476  0.332391  0.586591  0.719378  0.612262   \n",
       "6  0.301617  0.020043  0.918485  0.445900  0.976656  0.035959  0.994657   \n",
       "7  0.418199  0.767178  0.065719  0.423229  0.624257  0.799378  0.572715   \n",
       "8  0.303557  0.328762  0.829169  0.212381  0.988171  0.146131  0.885027   \n",
       "9  0.299323  0.634118  0.094413  0.488458  0.636429  0.557253  0.248635   \n",
       "\n",
       "   feature8  feature9  feature10   ...    feature13  feature14  feature15  \\\n",
       "0  0.864658  0.850340   0.116827   ...     0.976095   0.523960   0.148403   \n",
       "1  0.375746  0.927066   0.091658   ...     0.081815   0.128766   0.076859   \n",
       "2  0.643769  0.108549   0.293277   ...     0.943610   0.822201   0.333061   \n",
       "3  0.801898  0.133652   0.573551   ...     0.930512   0.853767   0.317888   \n",
       "4  0.623804  0.348972   0.715115   ...     0.671845   0.324413   0.856480   \n",
       "5  0.709459  0.487905   0.938784   ...     0.347437   0.016488   0.746829   \n",
       "6  0.112611  0.691903   0.823239   ...     0.008203   0.296781   0.959141   \n",
       "7  0.767760  0.619077   0.191164   ...     0.685799   0.517264   0.178209   \n",
       "8  0.185728  0.188507   0.981038   ...     0.384778   0.327223   0.983245   \n",
       "9  0.981777  0.872428   0.002076   ...     0.569149   0.438558   0.007408   \n",
       "\n",
       "   feature16  feature17  feature18  feature19  feature20  feature21  target  \n",
       "0   0.600202   0.688142   0.470819   0.286410   0.796651   0.641891       1  \n",
       "1   0.839097   0.576018   0.759510   0.745820   0.303073   0.878545       0  \n",
       "2   0.782492   0.666455   0.007921   0.608245   0.652344   0.033444       1  \n",
       "3   0.460311   0.063970   0.434131   0.202171   0.906957   0.297918       1  \n",
       "4   0.276030   0.231954   0.333042   0.517633   0.269451   0.320914       1  \n",
       "5   0.393725   0.658273   0.969089   0.552002   0.707005   0.762388       1  \n",
       "6   0.976805   0.592507   0.604772   0.538080   0.000875   0.611522       0  \n",
       "7   0.595161   0.841767   0.486965   0.813587   0.658656   0.646261       1  \n",
       "8   0.924070   0.080164   0.333042   0.116667   0.206736   0.259746       0  \n",
       "9   0.282386   0.395210   0.723274   0.222967   0.853229   0.782230       0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputCSV = pd.read_csv('numerai_training_data.csv')\n",
    "inputCSV.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48473</th>\n",
       "      <td>0.267237</td>\n",
       "      <td>0.264007</td>\n",
       "      <td>0.116871</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.928022</td>\n",
       "      <td>0.068729</td>\n",
       "      <td>0.628379</td>\n",
       "      <td>0.559896</td>\n",
       "      <td>0.971765</td>\n",
       "      <td>0.610268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151081</td>\n",
       "      <td>0.371125</td>\n",
       "      <td>0.528316</td>\n",
       "      <td>0.911122</td>\n",
       "      <td>0.360028</td>\n",
       "      <td>0.367189</td>\n",
       "      <td>0.568665</td>\n",
       "      <td>0.368693</td>\n",
       "      <td>0.457101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34953</th>\n",
       "      <td>0.289866</td>\n",
       "      <td>0.861914</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.444875</td>\n",
       "      <td>0.181340</td>\n",
       "      <td>0.858578</td>\n",
       "      <td>0.104368</td>\n",
       "      <td>0.587745</td>\n",
       "      <td>0.549819</td>\n",
       "      <td>0.715115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713052</td>\n",
       "      <td>0.410386</td>\n",
       "      <td>0.636224</td>\n",
       "      <td>0.260473</td>\n",
       "      <td>0.599239</td>\n",
       "      <td>0.160721</td>\n",
       "      <td>0.452185</td>\n",
       "      <td>0.468886</td>\n",
       "      <td>0.237294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60858</th>\n",
       "      <td>0.244921</td>\n",
       "      <td>0.549913</td>\n",
       "      <td>0.600992</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.239297</td>\n",
       "      <td>0.462402</td>\n",
       "      <td>0.018159</td>\n",
       "      <td>0.612390</td>\n",
       "      <td>0.336934</td>\n",
       "      <td>0.852291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261007</td>\n",
       "      <td>0.354061</td>\n",
       "      <td>0.771981</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>0.265608</td>\n",
       "      <td>0.099969</td>\n",
       "      <td>0.437955</td>\n",
       "      <td>0.433236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37041</th>\n",
       "      <td>0.274483</td>\n",
       "      <td>0.111568</td>\n",
       "      <td>0.044299</td>\n",
       "      <td>0.233717</td>\n",
       "      <td>0.399857</td>\n",
       "      <td>0.217382</td>\n",
       "      <td>0.341818</td>\n",
       "      <td>0.959074</td>\n",
       "      <td>0.787275</td>\n",
       "      <td>0.129955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179476</td>\n",
       "      <td>0.218583</td>\n",
       "      <td>0.044701</td>\n",
       "      <td>0.339477</td>\n",
       "      <td>0.234977</td>\n",
       "      <td>0.353886</td>\n",
       "      <td>0.160570</td>\n",
       "      <td>0.945028</td>\n",
       "      <td>0.715157</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14422</th>\n",
       "      <td>0.563806</td>\n",
       "      <td>0.085910</td>\n",
       "      <td>0.354630</td>\n",
       "      <td>0.425055</td>\n",
       "      <td>0.711377</td>\n",
       "      <td>0.411280</td>\n",
       "      <td>0.742378</td>\n",
       "      <td>0.368013</td>\n",
       "      <td>0.322918</td>\n",
       "      <td>0.290961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201743</td>\n",
       "      <td>0.392445</td>\n",
       "      <td>0.662487</td>\n",
       "      <td>0.777398</td>\n",
       "      <td>0.308180</td>\n",
       "      <td>0.585882</td>\n",
       "      <td>0.574256</td>\n",
       "      <td>0.371864</td>\n",
       "      <td>0.451790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32729</th>\n",
       "      <td>0.808552</td>\n",
       "      <td>0.937504</td>\n",
       "      <td>0.577363</td>\n",
       "      <td>0.725515</td>\n",
       "      <td>0.665610</td>\n",
       "      <td>0.982281</td>\n",
       "      <td>0.369034</td>\n",
       "      <td>0.570566</td>\n",
       "      <td>0.204822</td>\n",
       "      <td>0.442739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809670</td>\n",
       "      <td>0.788433</td>\n",
       "      <td>0.576929</td>\n",
       "      <td>0.534686</td>\n",
       "      <td>0.163943</td>\n",
       "      <td>0.225236</td>\n",
       "      <td>0.206333</td>\n",
       "      <td>0.554527</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85191</th>\n",
       "      <td>0.283136</td>\n",
       "      <td>0.217313</td>\n",
       "      <td>0.257578</td>\n",
       "      <td>0.208455</td>\n",
       "      <td>0.803595</td>\n",
       "      <td>0.237082</td>\n",
       "      <td>0.728964</td>\n",
       "      <td>0.233684</td>\n",
       "      <td>0.983227</td>\n",
       "      <td>0.071147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266642</td>\n",
       "      <td>0.099063</td>\n",
       "      <td>0.210823</td>\n",
       "      <td>0.570068</td>\n",
       "      <td>0.621894</td>\n",
       "      <td>0.377919</td>\n",
       "      <td>0.768661</td>\n",
       "      <td>0.360165</td>\n",
       "      <td>0.409194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78786</th>\n",
       "      <td>0.657740</td>\n",
       "      <td>0.812972</td>\n",
       "      <td>0.398299</td>\n",
       "      <td>0.747423</td>\n",
       "      <td>0.642095</td>\n",
       "      <td>0.715388</td>\n",
       "      <td>0.349160</td>\n",
       "      <td>0.229968</td>\n",
       "      <td>0.505458</td>\n",
       "      <td>0.390660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685799</td>\n",
       "      <td>0.810836</td>\n",
       "      <td>0.499223</td>\n",
       "      <td>0.338154</td>\n",
       "      <td>0.355965</td>\n",
       "      <td>0.358662</td>\n",
       "      <td>0.298091</td>\n",
       "      <td>0.462977</td>\n",
       "      <td>0.473590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24399</th>\n",
       "      <td>0.181035</td>\n",
       "      <td>0.703914</td>\n",
       "      <td>0.184222</td>\n",
       "      <td>0.212269</td>\n",
       "      <td>0.715664</td>\n",
       "      <td>0.257961</td>\n",
       "      <td>0.821313</td>\n",
       "      <td>0.141390</td>\n",
       "      <td>0.818261</td>\n",
       "      <td>0.056567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159951</td>\n",
       "      <td>0.595791</td>\n",
       "      <td>0.149229</td>\n",
       "      <td>0.615686</td>\n",
       "      <td>0.255971</td>\n",
       "      <td>0.745703</td>\n",
       "      <td>0.585825</td>\n",
       "      <td>0.418035</td>\n",
       "      <td>0.950710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77574</th>\n",
       "      <td>0.206047</td>\n",
       "      <td>0.306965</td>\n",
       "      <td>0.344082</td>\n",
       "      <td>0.348124</td>\n",
       "      <td>0.633733</td>\n",
       "      <td>0.647077</td>\n",
       "      <td>0.687997</td>\n",
       "      <td>0.786589</td>\n",
       "      <td>0.766099</td>\n",
       "      <td>0.299951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391576</td>\n",
       "      <td>0.191053</td>\n",
       "      <td>0.353461</td>\n",
       "      <td>0.669273</td>\n",
       "      <td>0.308180</td>\n",
       "      <td>0.697753</td>\n",
       "      <td>0.528111</td>\n",
       "      <td>0.487878</td>\n",
       "      <td>0.682311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "48473  0.267237  0.264007  0.116871  0.000138  0.928022  0.068729  0.628379   \n",
       "34953  0.289866  0.861914  0.249400  0.444875  0.181340  0.858578  0.104368   \n",
       "60858  0.244921  0.549913  0.600992  0.489465  0.239297  0.462402  0.018159   \n",
       "37041  0.274483  0.111568  0.044299  0.233717  0.399857  0.217382  0.341818   \n",
       "14422  0.563806  0.085910  0.354630  0.425055  0.711377  0.411280  0.742378   \n",
       "32729  0.808552  0.937504  0.577363  0.725515  0.665610  0.982281  0.369034   \n",
       "85191  0.283136  0.217313  0.257578  0.208455  0.803595  0.237082  0.728964   \n",
       "78786  0.657740  0.812972  0.398299  0.747423  0.642095  0.715388  0.349160   \n",
       "24399  0.181035  0.703914  0.184222  0.212269  0.715664  0.257961  0.821313   \n",
       "77574  0.206047  0.306965  0.344082  0.348124  0.633733  0.647077  0.687997   \n",
       "\n",
       "       feature8  feature9  feature10   ...    feature13  feature14  feature15  \\\n",
       "48473  0.559896  0.971765   0.610268   ...     0.151081   0.371125   0.528316   \n",
       "34953  0.587745  0.549819   0.715115   ...     0.713052   0.410386   0.636224   \n",
       "60858  0.612390  0.336934   0.852291   ...     0.261007   0.354061   0.771981   \n",
       "37041  0.959074  0.787275   0.129955   ...     0.179476   0.218583   0.044701   \n",
       "14422  0.368013  0.322918   0.290961   ...     0.201743   0.392445   0.662487   \n",
       "32729  0.570566  0.204822   0.442739   ...     0.809670   0.788433   0.576929   \n",
       "85191  0.233684  0.983227   0.071147   ...     0.266642   0.099063   0.210823   \n",
       "78786  0.229968  0.505458   0.390660   ...     0.685799   0.810836   0.499223   \n",
       "24399  0.141390  0.818261   0.056567   ...     0.159951   0.595791   0.149229   \n",
       "77574  0.786589  0.766099   0.299951   ...     0.391576   0.191053   0.353461   \n",
       "\n",
       "       feature16  feature17  feature18  feature19  feature20  feature21  \\\n",
       "48473   0.911122   0.360028   0.367189   0.568665   0.368693   0.457101   \n",
       "34953   0.260473   0.599239   0.160721   0.452185   0.468886   0.237294   \n",
       "60858   0.003988   0.019908   0.265608   0.099969   0.437955   0.433236   \n",
       "37041   0.339477   0.234977   0.353886   0.160570   0.945028   0.715157   \n",
       "14422   0.777398   0.308180   0.585882   0.574256   0.371864   0.451790   \n",
       "32729   0.534686   0.163943   0.225236   0.206333   0.554527   0.233900   \n",
       "85191   0.570068   0.621894   0.377919   0.768661   0.360165   0.409194   \n",
       "78786   0.338154   0.355965   0.358662   0.298091   0.462977   0.473590   \n",
       "24399   0.615686   0.255971   0.745703   0.585825   0.418035   0.950710   \n",
       "77574   0.669273   0.308180   0.697753   0.528111   0.487878   0.682311   \n",
       "\n",
       "       target  \n",
       "48473       0  \n",
       "34953       0  \n",
       "60858       0  \n",
       "37041       1  \n",
       "14422       0  \n",
       "32729       1  \n",
       "85191       0  \n",
       "78786       1  \n",
       "24399       0  \n",
       "77574       0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの並びをシャッフル\n",
    "calcData = inputCSV.reindex(np.random.permutation(inputCSV.index))\n",
    "calcData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 入力データと教師データに分割\n",
    "pd_y = calcData['target']\n",
    "pd_x = calcData.drop(['target'], axis=1)\n",
    "\n",
    "# numpy配列にデータフレームから変換\n",
    "x_array = pd_x.as_matrix()\n",
    "y_array = pd_y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96320"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohno/.pyenv/versions/3.5.1/envs/devkeras/lib/python3.5/site-packages/numpy/lib/shape_base.py:422: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  sub_arys.append(_nx.swapaxes(sary[st:end], axis, 0))\n"
     ]
    }
   ],
   "source": [
    "# 正規化し,float32の型に変換\n",
    "x_scale = scale(x_array).astype(np.float32)\n",
    "\n",
    "# サンプルの一部をテストデータ用に分割\n",
    "N = pd_x.shape[0] * 0.1\n",
    "X_train, X_test = np.split(x_scale, [N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師データのラベリング\n",
    "変換が必要な点に注意."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "nb_classes = 2 # 2値分類\n",
    "nb_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohno/.pyenv/versions/3.5.1/envs/devkeras/lib/python3.5/site-packages/numpy/lib/shape_base.py:422: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  sub_arys.append(_nx.swapaxes(sary[st:end], axis, 0))\n"
     ]
    }
   ],
   "source": [
    "y_scale = y_array.astype(np.int32)\n",
    "Y_train, Y_test = np.split(y_scale, [N])\n",
    "\n",
    "# 離散の分類問題の場合、ベクトルのラベルをto_categoricalで変換しておく必要がある.\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learningのモデル定義\n",
    "\n",
    "以下では暫定的に多層ニューラルネットを用いる.\n",
    "\n",
    "(ここが基本的に変更する場所である.)\n",
    "\n",
    "なお、活性化関数にReLUを用いる.\n",
    "\n",
    "また途中に過学習を防ぎ汎化性能を上げる手法であるdropoutが仕込んである."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(21,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルは一度コンパイルする形式を取るタイプ\n",
    "\n",
    "サマリーが出てちょっと見やすい."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_22 (Dense)                 (None, 100)           2200        dense_input_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 100)           0           dense_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 100)           0           activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 100)           10100       dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 100)           0           dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 100)           0           activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 100)           10100       dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 100)           0           dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 100)           0           activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 2)             202         dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 2)             0           dense_25[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 22602\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習と結果\n",
    "\n",
    "fitで学習し,evaluateでtestデータから汎化性能をチェックする."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9632 samples, validate on 86688 samples\n",
      "Epoch 1/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.7061 - acc: 0.4918 - val_loss: 0.6942 - val_acc: 0.5064\n",
      "Epoch 2/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6970 - acc: 0.5113 - val_loss: 0.6940 - val_acc: 0.5026\n",
      "Epoch 3/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6968 - acc: 0.5062 - val_loss: 0.6931 - val_acc: 0.5123\n",
      "Epoch 4/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6948 - acc: 0.5110 - val_loss: 0.6935 - val_acc: 0.5049\n",
      "Epoch 5/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6956 - acc: 0.5065 - val_loss: 0.6929 - val_acc: 0.5134\n",
      "Epoch 6/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6941 - acc: 0.5130 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 7/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6929 - acc: 0.5206 - val_loss: 0.6929 - val_acc: 0.5125\n",
      "Epoch 8/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6927 - acc: 0.5113 - val_loss: 0.6928 - val_acc: 0.5133\n",
      "Epoch 9/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6918 - acc: 0.5264 - val_loss: 0.6930 - val_acc: 0.5106\n",
      "Epoch 10/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6925 - acc: 0.5249 - val_loss: 0.6929 - val_acc: 0.5104\n",
      "Epoch 11/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6926 - acc: 0.5246 - val_loss: 0.6928 - val_acc: 0.5134\n",
      "Epoch 12/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6920 - acc: 0.5289 - val_loss: 0.6931 - val_acc: 0.5106\n",
      "Epoch 13/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6913 - acc: 0.5293 - val_loss: 0.6929 - val_acc: 0.5125\n",
      "Epoch 14/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6905 - acc: 0.5295 - val_loss: 0.6929 - val_acc: 0.5118\n",
      "Epoch 15/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6910 - acc: 0.5303 - val_loss: 0.6931 - val_acc: 0.5103\n",
      "Epoch 16/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6902 - acc: 0.5334 - val_loss: 0.6931 - val_acc: 0.5121\n",
      "Epoch 17/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6902 - acc: 0.5331 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 18/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6906 - acc: 0.5310 - val_loss: 0.6930 - val_acc: 0.5133\n",
      "Epoch 19/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6901 - acc: 0.5300 - val_loss: 0.6930 - val_acc: 0.5131\n",
      "Epoch 20/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6900 - acc: 0.5341 - val_loss: 0.6936 - val_acc: 0.5105\n",
      "Epoch 21/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6885 - acc: 0.5440 - val_loss: 0.6931 - val_acc: 0.5129\n",
      "Epoch 22/200\n",
      "9632/9632 [==============================] - 1s - loss: 0.6886 - acc: 0.5437 - val_loss: 0.6938 - val_acc: 0.5083\n",
      "Epoch 23/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6886 - acc: 0.5471 - val_loss: 0.6933 - val_acc: 0.5125\n",
      "Epoch 24/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6888 - acc: 0.5408 - val_loss: 0.6934 - val_acc: 0.5123\n",
      "Epoch 25/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6888 - acc: 0.5344 - val_loss: 0.6938 - val_acc: 0.5089\n",
      "Epoch 26/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6874 - acc: 0.5464 - val_loss: 0.6944 - val_acc: 0.5083\n",
      "Epoch 27/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6877 - acc: 0.5399 - val_loss: 0.6935 - val_acc: 0.5127\n",
      "Epoch 28/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6881 - acc: 0.5420 - val_loss: 0.6938 - val_acc: 0.5117\n",
      "Epoch 29/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6871 - acc: 0.5431 - val_loss: 0.6940 - val_acc: 0.5121\n",
      "Epoch 30/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6862 - acc: 0.5413 - val_loss: 0.6940 - val_acc: 0.5127\n",
      "Epoch 31/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6864 - acc: 0.5465 - val_loss: 0.6944 - val_acc: 0.5112\n",
      "Epoch 32/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6858 - acc: 0.5512 - val_loss: 0.6943 - val_acc: 0.5119\n",
      "Epoch 33/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6861 - acc: 0.5461 - val_loss: 0.6942 - val_acc: 0.5109\n",
      "Epoch 34/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6854 - acc: 0.5492 - val_loss: 0.6949 - val_acc: 0.5111\n",
      "Epoch 35/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6853 - acc: 0.5504 - val_loss: 0.6946 - val_acc: 0.5113\n",
      "Epoch 36/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6852 - acc: 0.5462 - val_loss: 0.6953 - val_acc: 0.5095\n",
      "Epoch 37/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6858 - acc: 0.5516 - val_loss: 0.6947 - val_acc: 0.5118\n",
      "Epoch 38/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6838 - acc: 0.5561 - val_loss: 0.6952 - val_acc: 0.5091\n",
      "Epoch 39/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6851 - acc: 0.5466 - val_loss: 0.6947 - val_acc: 0.5127\n",
      "Epoch 40/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6831 - acc: 0.5645 - val_loss: 0.6953 - val_acc: 0.5098\n",
      "Epoch 41/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6838 - acc: 0.5510 - val_loss: 0.6958 - val_acc: 0.5091\n",
      "Epoch 42/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6832 - acc: 0.5591 - val_loss: 0.6952 - val_acc: 0.5107\n",
      "Epoch 43/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6830 - acc: 0.5616 - val_loss: 0.6956 - val_acc: 0.5092\n",
      "Epoch 44/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6817 - acc: 0.5600 - val_loss: 0.6966 - val_acc: 0.5081\n",
      "Epoch 45/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6821 - acc: 0.5609 - val_loss: 0.6967 - val_acc: 0.5068\n",
      "Epoch 46/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6806 - acc: 0.5619 - val_loss: 0.6961 - val_acc: 0.5088\n",
      "Epoch 47/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6808 - acc: 0.5649 - val_loss: 0.6967 - val_acc: 0.5077\n",
      "Epoch 48/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6827 - acc: 0.5619 - val_loss: 0.6965 - val_acc: 0.5079\n",
      "Epoch 49/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6814 - acc: 0.5636 - val_loss: 0.6967 - val_acc: 0.5077\n",
      "Epoch 50/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6814 - acc: 0.5633 - val_loss: 0.6973 - val_acc: 0.5067\n",
      "Epoch 51/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6798 - acc: 0.5656 - val_loss: 0.6973 - val_acc: 0.5089\n",
      "Epoch 52/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6814 - acc: 0.5690 - val_loss: 0.6973 - val_acc: 0.5088\n",
      "Epoch 53/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6796 - acc: 0.5638 - val_loss: 0.6973 - val_acc: 0.5087\n",
      "Epoch 54/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6781 - acc: 0.5646 - val_loss: 0.6984 - val_acc: 0.5075\n",
      "Epoch 55/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6771 - acc: 0.5679 - val_loss: 0.6983 - val_acc: 0.5091\n",
      "Epoch 56/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6774 - acc: 0.5725 - val_loss: 0.6984 - val_acc: 0.5094\n",
      "Epoch 57/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6771 - acc: 0.5769 - val_loss: 0.6988 - val_acc: 0.5069\n",
      "Epoch 58/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6766 - acc: 0.5751 - val_loss: 0.6979 - val_acc: 0.5091\n",
      "Epoch 59/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6784 - acc: 0.5722 - val_loss: 0.6982 - val_acc: 0.5076\n",
      "Epoch 60/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6776 - acc: 0.5702 - val_loss: 0.6983 - val_acc: 0.5072\n",
      "Epoch 61/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6770 - acc: 0.5731 - val_loss: 0.6981 - val_acc: 0.5091\n",
      "Epoch 62/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6782 - acc: 0.5736 - val_loss: 0.6987 - val_acc: 0.5076\n",
      "Epoch 63/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6748 - acc: 0.5746 - val_loss: 0.6991 - val_acc: 0.5075\n",
      "Epoch 64/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6742 - acc: 0.5788 - val_loss: 0.6991 - val_acc: 0.5103\n",
      "Epoch 65/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6765 - acc: 0.5746 - val_loss: 0.6991 - val_acc: 0.5074\n",
      "Epoch 66/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6756 - acc: 0.5786 - val_loss: 0.6999 - val_acc: 0.5070\n",
      "Epoch 67/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6754 - acc: 0.5722 - val_loss: 0.6997 - val_acc: 0.5068\n",
      "Epoch 68/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6748 - acc: 0.5857 - val_loss: 0.6996 - val_acc: 0.5080\n",
      "Epoch 69/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6726 - acc: 0.5785 - val_loss: 0.7003 - val_acc: 0.5080\n",
      "Epoch 70/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6728 - acc: 0.5847 - val_loss: 0.7030 - val_acc: 0.5065\n",
      "Epoch 71/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6734 - acc: 0.5771 - val_loss: 0.6999 - val_acc: 0.5088\n",
      "Epoch 72/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6716 - acc: 0.5863 - val_loss: 0.7019 - val_acc: 0.5060\n",
      "Epoch 73/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6716 - acc: 0.5864 - val_loss: 0.7007 - val_acc: 0.5085\n",
      "Epoch 74/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6692 - acc: 0.5844 - val_loss: 0.7010 - val_acc: 0.5086\n",
      "Epoch 75/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6721 - acc: 0.5837 - val_loss: 0.7027 - val_acc: 0.5058\n",
      "Epoch 76/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6679 - acc: 0.5907 - val_loss: 0.7023 - val_acc: 0.5076\n",
      "Epoch 77/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6703 - acc: 0.5871 - val_loss: 0.7018 - val_acc: 0.5092\n",
      "Epoch 78/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6697 - acc: 0.5834 - val_loss: 0.7020 - val_acc: 0.5078\n",
      "Epoch 79/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6716 - acc: 0.5887 - val_loss: 0.7020 - val_acc: 0.5086\n",
      "Epoch 80/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6711 - acc: 0.5842 - val_loss: 0.7022 - val_acc: 0.5075\n",
      "Epoch 81/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6698 - acc: 0.5855 - val_loss: 0.7024 - val_acc: 0.5071\n",
      "Epoch 82/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6683 - acc: 0.5890 - val_loss: 0.7033 - val_acc: 0.5061\n",
      "Epoch 83/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6669 - acc: 0.5903 - val_loss: 0.7039 - val_acc: 0.5066\n",
      "Epoch 84/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6663 - acc: 0.5959 - val_loss: 0.7037 - val_acc: 0.5081\n",
      "Epoch 85/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6677 - acc: 0.5923 - val_loss: 0.7059 - val_acc: 0.5054\n",
      "Epoch 86/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6689 - acc: 0.5882 - val_loss: 0.7049 - val_acc: 0.5065\n",
      "Epoch 87/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6669 - acc: 0.5928 - val_loss: 0.7036 - val_acc: 0.5061\n",
      "Epoch 88/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6674 - acc: 0.5947 - val_loss: 0.7041 - val_acc: 0.5060\n",
      "Epoch 89/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6654 - acc: 0.5909 - val_loss: 0.7047 - val_acc: 0.5079\n",
      "Epoch 90/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6659 - acc: 0.5922 - val_loss: 0.7048 - val_acc: 0.5075\n",
      "Epoch 91/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6645 - acc: 0.5959 - val_loss: 0.7049 - val_acc: 0.5057\n",
      "Epoch 92/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6625 - acc: 0.5986 - val_loss: 0.7059 - val_acc: 0.5064\n",
      "Epoch 93/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6639 - acc: 0.5922 - val_loss: 0.7062 - val_acc: 0.5064\n",
      "Epoch 94/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6638 - acc: 0.5951 - val_loss: 0.7052 - val_acc: 0.5067\n",
      "Epoch 95/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6640 - acc: 0.5967 - val_loss: 0.7055 - val_acc: 0.5064\n",
      "Epoch 96/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6627 - acc: 0.5969 - val_loss: 0.7052 - val_acc: 0.5070\n",
      "Epoch 97/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6626 - acc: 0.6007 - val_loss: 0.7056 - val_acc: 0.5058\n",
      "Epoch 98/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6619 - acc: 0.5964 - val_loss: 0.7064 - val_acc: 0.5056\n",
      "Epoch 99/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6621 - acc: 0.6000 - val_loss: 0.7071 - val_acc: 0.5063\n",
      "Epoch 100/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6628 - acc: 0.6025 - val_loss: 0.7069 - val_acc: 0.5064\n",
      "Epoch 101/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6614 - acc: 0.6064 - val_loss: 0.7069 - val_acc: 0.5056\n",
      "Epoch 102/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6614 - acc: 0.6039 - val_loss: 0.7078 - val_acc: 0.5066\n",
      "Epoch 103/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6640 - acc: 0.5975 - val_loss: 0.7066 - val_acc: 0.5065\n",
      "Epoch 104/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6576 - acc: 0.6050 - val_loss: 0.7082 - val_acc: 0.5064\n",
      "Epoch 105/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6571 - acc: 0.6069 - val_loss: 0.7093 - val_acc: 0.5077\n",
      "Epoch 106/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6609 - acc: 0.6010 - val_loss: 0.7075 - val_acc: 0.5075\n",
      "Epoch 107/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6593 - acc: 0.6060 - val_loss: 0.7083 - val_acc: 0.5075\n",
      "Epoch 108/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6568 - acc: 0.6068 - val_loss: 0.7096 - val_acc: 0.5077\n",
      "Epoch 109/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6602 - acc: 0.5982 - val_loss: 0.7098 - val_acc: 0.5059\n",
      "Epoch 110/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6563 - acc: 0.6121 - val_loss: 0.7112 - val_acc: 0.5069\n",
      "Epoch 111/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6554 - acc: 0.6122 - val_loss: 0.7090 - val_acc: 0.5075\n",
      "Epoch 112/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6590 - acc: 0.6064 - val_loss: 0.7083 - val_acc: 0.5061\n",
      "Epoch 113/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6562 - acc: 0.6097 - val_loss: 0.7101 - val_acc: 0.5055\n",
      "Epoch 114/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6560 - acc: 0.6087 - val_loss: 0.7089 - val_acc: 0.5076\n",
      "Epoch 115/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6589 - acc: 0.6010 - val_loss: 0.7095 - val_acc: 0.5077\n",
      "Epoch 116/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6544 - acc: 0.6075 - val_loss: 0.7102 - val_acc: 0.5084\n",
      "Epoch 117/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6525 - acc: 0.6150 - val_loss: 0.7108 - val_acc: 0.5077\n",
      "Epoch 118/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6518 - acc: 0.6186 - val_loss: 0.7119 - val_acc: 0.5078\n",
      "Epoch 119/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6574 - acc: 0.6038 - val_loss: 0.7097 - val_acc: 0.5089\n",
      "Epoch 120/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6536 - acc: 0.6090 - val_loss: 0.7098 - val_acc: 0.5088\n",
      "Epoch 121/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6557 - acc: 0.6091 - val_loss: 0.7094 - val_acc: 0.5081\n",
      "Epoch 122/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6520 - acc: 0.6078 - val_loss: 0.7109 - val_acc: 0.5073\n",
      "Epoch 123/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6553 - acc: 0.6141 - val_loss: 0.7110 - val_acc: 0.5072\n",
      "Epoch 124/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6529 - acc: 0.6197 - val_loss: 0.7119 - val_acc: 0.5078\n",
      "Epoch 125/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6524 - acc: 0.6133 - val_loss: 0.7119 - val_acc: 0.5067\n",
      "Epoch 126/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6535 - acc: 0.6161 - val_loss: 0.7111 - val_acc: 0.5092\n",
      "Epoch 127/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6500 - acc: 0.6089 - val_loss: 0.7121 - val_acc: 0.5079\n",
      "Epoch 128/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6526 - acc: 0.6116 - val_loss: 0.7141 - val_acc: 0.5062\n",
      "Epoch 129/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6535 - acc: 0.6102 - val_loss: 0.7127 - val_acc: 0.5076\n",
      "Epoch 130/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6496 - acc: 0.6152 - val_loss: 0.7128 - val_acc: 0.5069\n",
      "Epoch 131/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6520 - acc: 0.6125 - val_loss: 0.7126 - val_acc: 0.5076\n",
      "Epoch 132/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6551 - acc: 0.6113 - val_loss: 0.7120 - val_acc: 0.5078\n",
      "Epoch 133/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6472 - acc: 0.6260 - val_loss: 0.7136 - val_acc: 0.5089\n",
      "Epoch 134/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6492 - acc: 0.6199 - val_loss: 0.7129 - val_acc: 0.5072\n",
      "Epoch 135/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6486 - acc: 0.6139 - val_loss: 0.7142 - val_acc: 0.5080\n",
      "Epoch 136/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6468 - acc: 0.6226 - val_loss: 0.7140 - val_acc: 0.5064\n",
      "Epoch 137/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6457 - acc: 0.6227 - val_loss: 0.7142 - val_acc: 0.5093\n",
      "Epoch 138/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6449 - acc: 0.6242 - val_loss: 0.7150 - val_acc: 0.5079\n",
      "Epoch 139/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6457 - acc: 0.6219 - val_loss: 0.7170 - val_acc: 0.5065\n",
      "Epoch 140/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6470 - acc: 0.6261 - val_loss: 0.7160 - val_acc: 0.5062\n",
      "Epoch 141/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6486 - acc: 0.6145 - val_loss: 0.7154 - val_acc: 0.5068\n",
      "Epoch 142/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6461 - acc: 0.6261 - val_loss: 0.7173 - val_acc: 0.5062\n",
      "Epoch 143/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6481 - acc: 0.6154 - val_loss: 0.7139 - val_acc: 0.5079\n",
      "Epoch 144/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6442 - acc: 0.6229 - val_loss: 0.7168 - val_acc: 0.5058\n",
      "Epoch 145/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6453 - acc: 0.6223 - val_loss: 0.7153 - val_acc: 0.5071\n",
      "Epoch 146/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6455 - acc: 0.6248 - val_loss: 0.7157 - val_acc: 0.5083\n",
      "Epoch 147/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6417 - acc: 0.6241 - val_loss: 0.7173 - val_acc: 0.5059\n",
      "Epoch 148/200\n",
      "9632/9632 [==============================] - 1s - loss: 0.6413 - acc: 0.6226 - val_loss: 0.7188 - val_acc: 0.5063\n",
      "Epoch 149/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6418 - acc: 0.6246 - val_loss: 0.7192 - val_acc: 0.5077\n",
      "Epoch 150/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6445 - acc: 0.6260 - val_loss: 0.7198 - val_acc: 0.5063\n",
      "Epoch 151/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6437 - acc: 0.6262 - val_loss: 0.7175 - val_acc: 0.5085\n",
      "Epoch 152/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6412 - acc: 0.6258 - val_loss: 0.7180 - val_acc: 0.5075\n",
      "Epoch 153/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6476 - acc: 0.6197 - val_loss: 0.7183 - val_acc: 0.5068\n",
      "Epoch 154/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6382 - acc: 0.6303 - val_loss: 0.7173 - val_acc: 0.5104\n",
      "Epoch 155/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6372 - acc: 0.6274 - val_loss: 0.7190 - val_acc: 0.5088\n",
      "Epoch 156/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6414 - acc: 0.6278 - val_loss: 0.7197 - val_acc: 0.5055\n",
      "Epoch 157/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6432 - acc: 0.6228 - val_loss: 0.7208 - val_acc: 0.5054\n",
      "Epoch 158/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6369 - acc: 0.6328 - val_loss: 0.7185 - val_acc: 0.5064\n",
      "Epoch 159/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6347 - acc: 0.6371 - val_loss: 0.7217 - val_acc: 0.5059\n",
      "Epoch 160/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6396 - acc: 0.6332 - val_loss: 0.7230 - val_acc: 0.5048\n",
      "Epoch 161/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6354 - acc: 0.6337 - val_loss: 0.7200 - val_acc: 0.5055\n",
      "Epoch 162/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6388 - acc: 0.6308 - val_loss: 0.7213 - val_acc: 0.5062\n",
      "Epoch 163/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6390 - acc: 0.6317 - val_loss: 0.7209 - val_acc: 0.5054\n",
      "Epoch 164/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6328 - acc: 0.6353 - val_loss: 0.7217 - val_acc: 0.5085\n",
      "Epoch 165/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6370 - acc: 0.6339 - val_loss: 0.7264 - val_acc: 0.5050\n",
      "Epoch 166/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6385 - acc: 0.6315 - val_loss: 0.7212 - val_acc: 0.5082\n",
      "Epoch 167/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6345 - acc: 0.6377 - val_loss: 0.7231 - val_acc: 0.5069\n",
      "Epoch 168/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6353 - acc: 0.6358 - val_loss: 0.7219 - val_acc: 0.5080\n",
      "Epoch 169/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6344 - acc: 0.6313 - val_loss: 0.7225 - val_acc: 0.5063\n",
      "Epoch 170/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6341 - acc: 0.6366 - val_loss: 0.7227 - val_acc: 0.5067\n",
      "Epoch 171/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6332 - acc: 0.6327 - val_loss: 0.7225 - val_acc: 0.5079\n",
      "Epoch 172/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6298 - acc: 0.6363 - val_loss: 0.7235 - val_acc: 0.5076\n",
      "Epoch 173/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6382 - acc: 0.6323 - val_loss: 0.7227 - val_acc: 0.5080\n",
      "Epoch 174/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6350 - acc: 0.6316 - val_loss: 0.7244 - val_acc: 0.5047\n",
      "Epoch 175/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6322 - acc: 0.6381 - val_loss: 0.7246 - val_acc: 0.5044\n",
      "Epoch 176/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6303 - acc: 0.6442 - val_loss: 0.7236 - val_acc: 0.5070\n",
      "Epoch 177/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6342 - acc: 0.6293 - val_loss: 0.7235 - val_acc: 0.5079\n",
      "Epoch 178/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6290 - acc: 0.6420 - val_loss: 0.7247 - val_acc: 0.5082\n",
      "Epoch 179/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6356 - acc: 0.6326 - val_loss: 0.7239 - val_acc: 0.5041\n",
      "Epoch 180/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6282 - acc: 0.6409 - val_loss: 0.7264 - val_acc: 0.5052\n",
      "Epoch 181/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6326 - acc: 0.6370 - val_loss: 0.7252 - val_acc: 0.5074\n",
      "Epoch 182/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6302 - acc: 0.6386 - val_loss: 0.7271 - val_acc: 0.5064\n",
      "Epoch 183/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6279 - acc: 0.6440 - val_loss: 0.7260 - val_acc: 0.5070\n",
      "Epoch 184/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6283 - acc: 0.6420 - val_loss: 0.7272 - val_acc: 0.5047\n",
      "Epoch 185/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6335 - acc: 0.6375 - val_loss: 0.7248 - val_acc: 0.5055\n",
      "Epoch 186/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6306 - acc: 0.6369 - val_loss: 0.7263 - val_acc: 0.5051\n",
      "Epoch 187/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6275 - acc: 0.6435 - val_loss: 0.7255 - val_acc: 0.5055\n",
      "Epoch 188/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6272 - acc: 0.6468 - val_loss: 0.7269 - val_acc: 0.5074\n",
      "Epoch 189/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6261 - acc: 0.6421 - val_loss: 0.7291 - val_acc: 0.5047\n",
      "Epoch 190/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6304 - acc: 0.6462 - val_loss: 0.7266 - val_acc: 0.5048\n",
      "Epoch 191/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6278 - acc: 0.6483 - val_loss: 0.7273 - val_acc: 0.5054\n",
      "Epoch 192/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6280 - acc: 0.6430 - val_loss: 0.7301 - val_acc: 0.5054\n",
      "Epoch 193/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6249 - acc: 0.6470 - val_loss: 0.7298 - val_acc: 0.5059\n",
      "Epoch 194/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6223 - acc: 0.6476 - val_loss: 0.7303 - val_acc: 0.5067\n",
      "Epoch 195/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6256 - acc: 0.6449 - val_loss: 0.7331 - val_acc: 0.5052\n",
      "Epoch 196/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6269 - acc: 0.6429 - val_loss: 0.7272 - val_acc: 0.5076\n",
      "Epoch 197/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6269 - acc: 0.6415 - val_loss: 0.7282 - val_acc: 0.5067\n",
      "Epoch 198/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6236 - acc: 0.6468 - val_loss: 0.7307 - val_acc: 0.5062\n",
      "Epoch 199/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6271 - acc: 0.6424 - val_loss: 0.7288 - val_acc: 0.5073\n",
      "Epoch 200/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6255 - acc: 0.6490 - val_loss: 0.7317 - val_acc: 0.5053\n",
      "Test score: 0.731741301905\n",
      "Test accuracy: 0.50530638612\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35127</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.296856</td>\n",
       "      <td>0.416888</td>\n",
       "      <td>0.271249</td>\n",
       "      <td>0.609466</td>\n",
       "      <td>0.272623</td>\n",
       "      <td>0.455763</td>\n",
       "      <td>0.719717</td>\n",
       "      <td>0.594638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612889</td>\n",
       "      <td>0.129737</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>0.717095</td>\n",
       "      <td>0.396762</td>\n",
       "      <td>0.572472</td>\n",
       "      <td>0.942516</td>\n",
       "      <td>0.549933</td>\n",
       "      <td>0.875827</td>\n",
       "      <td>0.866483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5815</td>\n",
       "      <td>0.945969</td>\n",
       "      <td>0.520152</td>\n",
       "      <td>0.913747</td>\n",
       "      <td>0.947708</td>\n",
       "      <td>0.876677</td>\n",
       "      <td>0.183711</td>\n",
       "      <td>0.994657</td>\n",
       "      <td>0.091799</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122915</td>\n",
       "      <td>0.314198</td>\n",
       "      <td>0.845424</td>\n",
       "      <td>0.932939</td>\n",
       "      <td>0.932285</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.040073</td>\n",
       "      <td>0.240286</td>\n",
       "      <td>0.252535</td>\n",
       "      <td>0.196575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21613</td>\n",
       "      <td>0.375753</td>\n",
       "      <td>0.131812</td>\n",
       "      <td>0.398299</td>\n",
       "      <td>0.714723</td>\n",
       "      <td>0.038811</td>\n",
       "      <td>0.286954</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>0.159726</td>\n",
       "      <td>0.176476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130703</td>\n",
       "      <td>0.088841</td>\n",
       "      <td>0.475284</td>\n",
       "      <td>0.145227</td>\n",
       "      <td>0.035899</td>\n",
       "      <td>0.056418</td>\n",
       "      <td>0.410543</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>0.066628</td>\n",
       "      <td>0.675992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24162</td>\n",
       "      <td>0.033064</td>\n",
       "      <td>0.131812</td>\n",
       "      <td>0.032016</td>\n",
       "      <td>0.141477</td>\n",
       "      <td>0.454620</td>\n",
       "      <td>0.462402</td>\n",
       "      <td>0.168206</td>\n",
       "      <td>0.171101</td>\n",
       "      <td>0.931317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707663</td>\n",
       "      <td>0.169385</td>\n",
       "      <td>0.340110</td>\n",
       "      <td>0.235432</td>\n",
       "      <td>0.061764</td>\n",
       "      <td>0.758716</td>\n",
       "      <td>0.843956</td>\n",
       "      <td>0.275956</td>\n",
       "      <td>0.070583</td>\n",
       "      <td>0.919119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13935</td>\n",
       "      <td>0.142782</td>\n",
       "      <td>0.071785</td>\n",
       "      <td>0.468688</td>\n",
       "      <td>0.408943</td>\n",
       "      <td>0.911702</td>\n",
       "      <td>0.101406</td>\n",
       "      <td>0.909998</td>\n",
       "      <td>0.223743</td>\n",
       "      <td>0.517424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967458</td>\n",
       "      <td>0.024189</td>\n",
       "      <td>0.060593</td>\n",
       "      <td>0.130316</td>\n",
       "      <td>0.834466</td>\n",
       "      <td>0.958453</td>\n",
       "      <td>0.933132</td>\n",
       "      <td>0.954030</td>\n",
       "      <td>0.143196</td>\n",
       "      <td>0.811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6769</td>\n",
       "      <td>0.664717</td>\n",
       "      <td>0.826183</td>\n",
       "      <td>0.276454</td>\n",
       "      <td>0.283183</td>\n",
       "      <td>0.119841</td>\n",
       "      <td>0.997214</td>\n",
       "      <td>0.424611</td>\n",
       "      <td>0.547748</td>\n",
       "      <td>0.851347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713724</td>\n",
       "      <td>0.882243</td>\n",
       "      <td>0.595791</td>\n",
       "      <td>0.421879</td>\n",
       "      <td>0.461548</td>\n",
       "      <td>0.371382</td>\n",
       "      <td>0.335522</td>\n",
       "      <td>0.530886</td>\n",
       "      <td>0.391486</td>\n",
       "      <td>0.361026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23101</td>\n",
       "      <td>0.584238</td>\n",
       "      <td>0.761102</td>\n",
       "      <td>0.724517</td>\n",
       "      <td>0.697422</td>\n",
       "      <td>0.576719</td>\n",
       "      <td>0.687706</td>\n",
       "      <td>0.647262</td>\n",
       "      <td>0.159726</td>\n",
       "      <td>0.389062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612459</td>\n",
       "      <td>0.718174</td>\n",
       "      <td>0.579461</td>\n",
       "      <td>0.806217</td>\n",
       "      <td>0.562815</td>\n",
       "      <td>0.625256</td>\n",
       "      <td>0.472099</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.109898</td>\n",
       "      <td>0.585839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27270</td>\n",
       "      <td>0.167520</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>0.331846</td>\n",
       "      <td>0.173939</td>\n",
       "      <td>0.819964</td>\n",
       "      <td>0.379880</td>\n",
       "      <td>0.512372</td>\n",
       "      <td>0.761694</td>\n",
       "      <td>0.594569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207911</td>\n",
       "      <td>0.334589</td>\n",
       "      <td>0.064694</td>\n",
       "      <td>0.389487</td>\n",
       "      <td>0.399285</td>\n",
       "      <td>0.124956</td>\n",
       "      <td>0.755826</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.438344</td>\n",
       "      <td>0.626302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2044</td>\n",
       "      <td>0.192413</td>\n",
       "      <td>0.229133</td>\n",
       "      <td>0.309713</td>\n",
       "      <td>0.173912</td>\n",
       "      <td>0.801884</td>\n",
       "      <td>0.259285</td>\n",
       "      <td>0.844189</td>\n",
       "      <td>0.143807</td>\n",
       "      <td>0.922540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572723</td>\n",
       "      <td>0.084591</td>\n",
       "      <td>0.372183</td>\n",
       "      <td>0.184446</td>\n",
       "      <td>0.441292</td>\n",
       "      <td>0.740264</td>\n",
       "      <td>0.200827</td>\n",
       "      <td>0.379106</td>\n",
       "      <td>0.135458</td>\n",
       "      <td>0.525279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10270</td>\n",
       "      <td>0.305077</td>\n",
       "      <td>0.526421</td>\n",
       "      <td>0.945373</td>\n",
       "      <td>0.585993</td>\n",
       "      <td>0.946542</td>\n",
       "      <td>0.273140</td>\n",
       "      <td>0.798628</td>\n",
       "      <td>0.207506</td>\n",
       "      <td>0.044341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919853</td>\n",
       "      <td>0.322999</td>\n",
       "      <td>0.343980</td>\n",
       "      <td>0.436126</td>\n",
       "      <td>0.929317</td>\n",
       "      <td>0.940058</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>0.952063</td>\n",
       "      <td>0.009844</td>\n",
       "      <td>0.617795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    t_id  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0  35127  0.093420  0.296856  0.416888  0.271249  0.609466  0.272623   \n",
       "1   5815  0.945969  0.520152  0.913747  0.947708  0.876677  0.183711   \n",
       "2  21613  0.375753  0.131812  0.398299  0.714723  0.038811  0.286954   \n",
       "3  24162  0.033064  0.131812  0.032016  0.141477  0.454620  0.462402   \n",
       "4  13935  0.142782  0.071785  0.468688  0.408943  0.911702  0.101406   \n",
       "5   6769  0.664717  0.826183  0.276454  0.283183  0.119841  0.997214   \n",
       "6  23101  0.584238  0.761102  0.724517  0.697422  0.576719  0.687706   \n",
       "7  27270  0.167520  0.051942  0.331846  0.173939  0.819964  0.379880   \n",
       "8   2044  0.192413  0.229133  0.309713  0.173912  0.801884  0.259285   \n",
       "9  10270  0.305077  0.526421  0.945373  0.585993  0.946542  0.273140   \n",
       "\n",
       "   feature7  feature8  feature9    ...      feature12  feature13  feature14  \\\n",
       "0  0.455763  0.719717  0.594638    ...       0.612889   0.129737   0.015982   \n",
       "1  0.994657  0.091799  0.176480    ...       0.122915   0.314198   0.845424   \n",
       "2  0.016110  0.159726  0.176476    ...       0.130703   0.088841   0.475284   \n",
       "3  0.168206  0.171101  0.931317    ...       0.707663   0.169385   0.340110   \n",
       "4  0.909998  0.223743  0.517424    ...       0.967458   0.024189   0.060593   \n",
       "5  0.424611  0.547748  0.851347    ...       0.713724   0.882243   0.595791   \n",
       "6  0.647262  0.159726  0.389062    ...       0.612459   0.718174   0.579461   \n",
       "7  0.512372  0.761694  0.594569    ...       0.207911   0.334589   0.064694   \n",
       "8  0.844189  0.143807  0.922540    ...       0.572723   0.084591   0.372183   \n",
       "9  0.798628  0.207506  0.044341    ...       0.919853   0.322999   0.343980   \n",
       "\n",
       "   feature15  feature16  feature17  feature18  feature19  feature20  feature21  \n",
       "0   0.717095   0.396762   0.572472   0.942516   0.549933   0.875827   0.866483  \n",
       "1   0.932939   0.932285   0.009922   0.040073   0.240286   0.252535   0.196575  \n",
       "2   0.145227   0.035899   0.056418   0.410543   0.061295   0.066628   0.675992  \n",
       "3   0.235432   0.061764   0.758716   0.843956   0.275956   0.070583   0.919119  \n",
       "4   0.130316   0.834466   0.958453   0.933132   0.954030   0.143196   0.811200  \n",
       "5   0.421879   0.461548   0.371382   0.335522   0.530886   0.391486   0.361026  \n",
       "6   0.806217   0.562815   0.625256   0.472099   0.670752   0.109898   0.585839  \n",
       "7   0.389487   0.399285   0.124956   0.755826   0.075363   0.438344   0.626302  \n",
       "8   0.184446   0.441292   0.740264   0.200827   0.379106   0.135458   0.525279  \n",
       "9   0.436126   0.929317   0.940058   0.033285   0.952063   0.009844   0.617795  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.read_csv(\"numerai_tournament_data.csv\")\n",
    "pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.296856</td>\n",
       "      <td>0.416888</td>\n",
       "      <td>0.271249</td>\n",
       "      <td>0.609466</td>\n",
       "      <td>0.272623</td>\n",
       "      <td>0.455763</td>\n",
       "      <td>0.719717</td>\n",
       "      <td>0.594638</td>\n",
       "      <td>0.916434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612889</td>\n",
       "      <td>0.129737</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>0.717095</td>\n",
       "      <td>0.396762</td>\n",
       "      <td>0.572472</td>\n",
       "      <td>0.942516</td>\n",
       "      <td>0.549933</td>\n",
       "      <td>0.875827</td>\n",
       "      <td>0.866483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.945969</td>\n",
       "      <td>0.520152</td>\n",
       "      <td>0.913747</td>\n",
       "      <td>0.947708</td>\n",
       "      <td>0.876677</td>\n",
       "      <td>0.183711</td>\n",
       "      <td>0.994657</td>\n",
       "      <td>0.091799</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>0.938784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122915</td>\n",
       "      <td>0.314198</td>\n",
       "      <td>0.845424</td>\n",
       "      <td>0.932939</td>\n",
       "      <td>0.932285</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.040073</td>\n",
       "      <td>0.240286</td>\n",
       "      <td>0.252535</td>\n",
       "      <td>0.196575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375753</td>\n",
       "      <td>0.131812</td>\n",
       "      <td>0.398299</td>\n",
       "      <td>0.714723</td>\n",
       "      <td>0.038811</td>\n",
       "      <td>0.286954</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>0.159726</td>\n",
       "      <td>0.176476</td>\n",
       "      <td>0.125447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130703</td>\n",
       "      <td>0.088841</td>\n",
       "      <td>0.475284</td>\n",
       "      <td>0.145227</td>\n",
       "      <td>0.035899</td>\n",
       "      <td>0.056418</td>\n",
       "      <td>0.410543</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>0.066628</td>\n",
       "      <td>0.675992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.093420  0.296856  0.416888  0.271249  0.609466  0.272623  0.455763   \n",
       "1  0.945969  0.520152  0.913747  0.947708  0.876677  0.183711  0.994657   \n",
       "2  0.375753  0.131812  0.398299  0.714723  0.038811  0.286954  0.016110   \n",
       "\n",
       "   feature8  feature9  feature10    ...      feature12  feature13  feature14  \\\n",
       "0  0.719717  0.594638   0.916434    ...       0.612889   0.129737   0.015982   \n",
       "1  0.091799  0.176480   0.938784    ...       0.122915   0.314198   0.845424   \n",
       "2  0.159726  0.176476   0.125447    ...       0.130703   0.088841   0.475284   \n",
       "\n",
       "   feature15  feature16  feature17  feature18  feature19  feature20  feature21  \n",
       "0   0.717095   0.396762   0.572472   0.942516   0.549933   0.875827   0.866483  \n",
       "1   0.932939   0.932285   0.009922   0.040073   0.240286   0.252535   0.196575  \n",
       "2   0.145227   0.035899   0.056418   0.410543   0.061295   0.066628   0.675992  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_x = pred.drop([\"t_id\"],axis=1)\n",
    "pred_x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_pred = pred_x.as_matrix()\n",
    "\n",
    "ans = model.predict(np_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.312819\n",
       "1        0.534440\n",
       "2        0.645487\n",
       "3        0.470157\n",
       "4        0.397954\n",
       "5        0.432818\n",
       "6        0.547083\n",
       "7        0.486919\n",
       "8        0.485804\n",
       "9        0.549066\n",
       "10       0.519676\n",
       "11       0.374994\n",
       "12       0.552595\n",
       "13       0.432911\n",
       "14       0.487856\n",
       "15       0.472630\n",
       "16       0.586581\n",
       "17       0.637877\n",
       "18       0.446020\n",
       "19       0.563931\n",
       "20       0.492324\n",
       "21       0.522402\n",
       "22       0.428217\n",
       "23       0.469244\n",
       "24       0.455476\n",
       "25       0.529906\n",
       "26       0.587196\n",
       "27       0.523455\n",
       "28       0.287439\n",
       "29       0.544257\n",
       "           ...   \n",
       "36042    0.418537\n",
       "36043    0.322587\n",
       "36044    0.430763\n",
       "36045    0.377407\n",
       "36046    0.450796\n",
       "36047    0.409155\n",
       "36048    0.554193\n",
       "36049    0.580082\n",
       "36050    0.448531\n",
       "36051    0.515618\n",
       "36052    0.488265\n",
       "36053    0.523377\n",
       "36054    0.409296\n",
       "36055    0.535402\n",
       "36056    0.435592\n",
       "36057    0.564455\n",
       "36058    0.428961\n",
       "36059    0.442477\n",
       "36060    0.542688\n",
       "36061    0.500820\n",
       "36062    0.466423\n",
       "36063    0.418483\n",
       "36064    0.470097\n",
       "36065    0.473675\n",
       "36066    0.498152\n",
       "36067    0.563094\n",
       "36068    0.430621\n",
       "36069    0.512041\n",
       "36070    0.491776\n",
       "36071    0.511618\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_ans = pd.DataFrame(ans)\n",
    "pd_ans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_csv = pd.concat([pred[\"t_id\"],pd_ans[1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35127</td>\n",
       "      <td>0.312819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5815</td>\n",
       "      <td>0.534440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21613</td>\n",
       "      <td>0.645487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24162</td>\n",
       "      <td>0.470157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13935</td>\n",
       "      <td>0.397954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6769</td>\n",
       "      <td>0.432818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23101</td>\n",
       "      <td>0.547083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27270</td>\n",
       "      <td>0.486919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2044</td>\n",
       "      <td>0.485804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10270</td>\n",
       "      <td>0.549066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    t_id         1\n",
       "0  35127  0.312819\n",
       "1   5815  0.534440\n",
       "2  21613  0.645487\n",
       "3  24162  0.470157\n",
       "4  13935  0.397954\n",
       "5   6769  0.432818\n",
       "6  23101  0.547083\n",
       "7  27270  0.486919\n",
       "8   2044  0.485804\n",
       "9  10270  0.549066"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_csv.to_csv(\"pred_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
