{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras基本例からNumerai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用データ\n",
    "\n",
    "同じフォルダにnumeraiのtrainingデータセットおくこと."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.801621</td>\n",
       "      <td>0.954027</td>\n",
       "      <td>0.570531</td>\n",
       "      <td>0.144233</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>0.078679</td>\n",
       "      <td>0.929508</td>\n",
       "      <td>0.171078</td>\n",
       "      <td>0.883034</td>\n",
       "      <td>0.179370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199107</td>\n",
       "      <td>0.968784</td>\n",
       "      <td>0.064581</td>\n",
       "      <td>0.916191</td>\n",
       "      <td>0.238189</td>\n",
       "      <td>0.093656</td>\n",
       "      <td>0.206537</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>0.659445</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.540071</td>\n",
       "      <td>0.459007</td>\n",
       "      <td>0.933132</td>\n",
       "      <td>0.499801</td>\n",
       "      <td>0.437993</td>\n",
       "      <td>0.267236</td>\n",
       "      <td>0.497195</td>\n",
       "      <td>0.456021</td>\n",
       "      <td>0.609753</td>\n",
       "      <td>0.187454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129508</td>\n",
       "      <td>0.479954</td>\n",
       "      <td>0.416888</td>\n",
       "      <td>0.634561</td>\n",
       "      <td>0.710521</td>\n",
       "      <td>0.324297</td>\n",
       "      <td>0.339858</td>\n",
       "      <td>0.349510</td>\n",
       "      <td>0.432921</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.518214</td>\n",
       "      <td>0.383788</td>\n",
       "      <td>0.424871</td>\n",
       "      <td>0.792763</td>\n",
       "      <td>0.774286</td>\n",
       "      <td>0.451385</td>\n",
       "      <td>0.791401</td>\n",
       "      <td>0.845301</td>\n",
       "      <td>0.118626</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266524</td>\n",
       "      <td>0.648017</td>\n",
       "      <td>0.420798</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.992130</td>\n",
       "      <td>0.718081</td>\n",
       "      <td>0.992094</td>\n",
       "      <td>0.134067</td>\n",
       "      <td>0.074308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.806454</td>\n",
       "      <td>0.161343</td>\n",
       "      <td>0.636976</td>\n",
       "      <td>0.201716</td>\n",
       "      <td>0.420538</td>\n",
       "      <td>0.151819</td>\n",
       "      <td>0.305147</td>\n",
       "      <td>0.117885</td>\n",
       "      <td>0.610892</td>\n",
       "      <td>0.249714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205248</td>\n",
       "      <td>0.086210</td>\n",
       "      <td>0.532110</td>\n",
       "      <td>0.790242</td>\n",
       "      <td>0.198782</td>\n",
       "      <td>0.274051</td>\n",
       "      <td>0.192057</td>\n",
       "      <td>0.097587</td>\n",
       "      <td>0.669891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.415003</td>\n",
       "      <td>0.170186</td>\n",
       "      <td>0.418823</td>\n",
       "      <td>0.471830</td>\n",
       "      <td>0.570765</td>\n",
       "      <td>0.213789</td>\n",
       "      <td>0.158155</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.418276</td>\n",
       "      <td>0.558754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506977</td>\n",
       "      <td>0.300006</td>\n",
       "      <td>0.742479</td>\n",
       "      <td>0.206817</td>\n",
       "      <td>0.814364</td>\n",
       "      <td>0.331907</td>\n",
       "      <td>0.065424</td>\n",
       "      <td>0.406144</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.704449</td>\n",
       "      <td>0.850339</td>\n",
       "      <td>0.455998</td>\n",
       "      <td>0.224563</td>\n",
       "      <td>0.699544</td>\n",
       "      <td>0.834721</td>\n",
       "      <td>0.747288</td>\n",
       "      <td>0.901346</td>\n",
       "      <td>0.497416</td>\n",
       "      <td>0.903353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834001</td>\n",
       "      <td>0.729096</td>\n",
       "      <td>0.810783</td>\n",
       "      <td>0.609452</td>\n",
       "      <td>0.192695</td>\n",
       "      <td>0.621665</td>\n",
       "      <td>0.737860</td>\n",
       "      <td>0.194186</td>\n",
       "      <td>0.364996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.788056</td>\n",
       "      <td>0.054596</td>\n",
       "      <td>0.200827</td>\n",
       "      <td>0.067378</td>\n",
       "      <td>0.941077</td>\n",
       "      <td>0.778948</td>\n",
       "      <td>0.116750</td>\n",
       "      <td>0.228003</td>\n",
       "      <td>0.136388</td>\n",
       "      <td>0.964285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784283</td>\n",
       "      <td>0.039394</td>\n",
       "      <td>0.933946</td>\n",
       "      <td>0.669043</td>\n",
       "      <td>0.138145</td>\n",
       "      <td>0.918439</td>\n",
       "      <td>0.880196</td>\n",
       "      <td>0.070342</td>\n",
       "      <td>0.479384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.188298</td>\n",
       "      <td>0.604075</td>\n",
       "      <td>0.018341</td>\n",
       "      <td>0.815766</td>\n",
       "      <td>0.518478</td>\n",
       "      <td>0.803098</td>\n",
       "      <td>0.711386</td>\n",
       "      <td>0.652814</td>\n",
       "      <td>0.052646</td>\n",
       "      <td>0.817774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730338</td>\n",
       "      <td>0.586008</td>\n",
       "      <td>0.416888</td>\n",
       "      <td>0.223151</td>\n",
       "      <td>0.626276</td>\n",
       "      <td>0.963009</td>\n",
       "      <td>0.689730</td>\n",
       "      <td>0.689253</td>\n",
       "      <td>0.132086</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.820315</td>\n",
       "      <td>0.637409</td>\n",
       "      <td>0.907437</td>\n",
       "      <td>0.243122</td>\n",
       "      <td>0.378766</td>\n",
       "      <td>0.257982</td>\n",
       "      <td>0.983338</td>\n",
       "      <td>0.355089</td>\n",
       "      <td>0.883293</td>\n",
       "      <td>0.123529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221233</td>\n",
       "      <td>0.812422</td>\n",
       "      <td>0.112858</td>\n",
       "      <td>0.709823</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.041301</td>\n",
       "      <td>0.500063</td>\n",
       "      <td>0.266616</td>\n",
       "      <td>0.735886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.798261</td>\n",
       "      <td>0.968608</td>\n",
       "      <td>0.902818</td>\n",
       "      <td>0.304248</td>\n",
       "      <td>0.574142</td>\n",
       "      <td>0.225799</td>\n",
       "      <td>0.767128</td>\n",
       "      <td>0.287060</td>\n",
       "      <td>0.726069</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048475</td>\n",
       "      <td>0.904284</td>\n",
       "      <td>0.648878</td>\n",
       "      <td>0.630587</td>\n",
       "      <td>0.344180</td>\n",
       "      <td>0.457721</td>\n",
       "      <td>0.339951</td>\n",
       "      <td>0.158732</td>\n",
       "      <td>0.706181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.801621  0.954027  0.570531  0.144233  0.050049  0.078679  0.929508   \n",
       "1  0.540071  0.459007  0.933132  0.499801  0.437993  0.267236  0.497195   \n",
       "2  0.518214  0.383788  0.424871  0.792763  0.774286  0.451385  0.791401   \n",
       "3  0.806454  0.161343  0.636976  0.201716  0.420538  0.151819  0.305147   \n",
       "4  0.415003  0.170186  0.418823  0.471830  0.570765  0.213789  0.158155   \n",
       "5  0.704449  0.850339  0.455998  0.224563  0.699544  0.834721  0.747288   \n",
       "6  0.788056  0.054596  0.200827  0.067378  0.941077  0.778948  0.116750   \n",
       "7  0.188298  0.604075  0.018341  0.815766  0.518478  0.803098  0.711386   \n",
       "8  0.820315  0.637409  0.907437  0.243122  0.378766  0.257982  0.983338   \n",
       "9  0.798261  0.968608  0.902818  0.304248  0.574142  0.225799  0.767128   \n",
       "\n",
       "   feature8  feature9  feature10   ...    feature13  feature14  feature15  \\\n",
       "0  0.171078  0.883034   0.179370   ...     0.199107   0.968784   0.064581   \n",
       "1  0.456021  0.609753   0.187454   ...     0.129508   0.479954   0.416888   \n",
       "2  0.845301  0.118626   0.419257   ...     0.266524   0.648017   0.420798   \n",
       "3  0.117885  0.610892   0.249714   ...     0.205248   0.086210   0.532110   \n",
       "4  0.011730  0.418276   0.558754   ...     0.506977   0.300006   0.742479   \n",
       "5  0.901346  0.497416   0.903353   ...     0.834001   0.729096   0.810783   \n",
       "6  0.228003  0.136388   0.964285   ...     0.784283   0.039394   0.933946   \n",
       "7  0.652814  0.052646   0.817774   ...     0.730338   0.586008   0.416888   \n",
       "8  0.355089  0.883293   0.123529   ...     0.221233   0.812422   0.112858   \n",
       "9  0.287060  0.726069   0.001582   ...     0.048475   0.904284   0.648878   \n",
       "\n",
       "   feature16  feature17  feature18  feature19  feature20  feature21  target  \n",
       "0   0.916191   0.238189   0.093656   0.206537   0.160289   0.659445       1  \n",
       "1   0.634561   0.710521   0.324297   0.339858   0.349510   0.432921       1  \n",
       "2   0.004319   0.992130   0.718081   0.992094   0.134067   0.074308       0  \n",
       "3   0.790242   0.198782   0.274051   0.192057   0.097587   0.669891       0  \n",
       "4   0.206817   0.814364   0.331907   0.065424   0.406144   0.746500       0  \n",
       "5   0.609452   0.192695   0.621665   0.737860   0.194186   0.364996       1  \n",
       "6   0.669043   0.138145   0.918439   0.880196   0.070342   0.479384       1  \n",
       "7   0.223151   0.626276   0.963009   0.689730   0.689253   0.132086       0  \n",
       "8   0.709823   0.598131   0.041301   0.500063   0.266616   0.735886       1  \n",
       "9   0.630587   0.344180   0.457721   0.339951   0.158732   0.706181       1  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputCSV = pd.read_csv('numerai_training_data.csv')\n",
    "inputCSV.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48473</th>\n",
       "      <td>0.443313</td>\n",
       "      <td>0.202047</td>\n",
       "      <td>0.433925</td>\n",
       "      <td>0.900588</td>\n",
       "      <td>0.676421</td>\n",
       "      <td>0.994305</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>0.922447</td>\n",
       "      <td>0.145433</td>\n",
       "      <td>0.815773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917480</td>\n",
       "      <td>0.028845</td>\n",
       "      <td>0.625409</td>\n",
       "      <td>0.391161</td>\n",
       "      <td>0.755612</td>\n",
       "      <td>0.842537</td>\n",
       "      <td>0.776601</td>\n",
       "      <td>0.962662</td>\n",
       "      <td>0.177580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34953</th>\n",
       "      <td>0.413209</td>\n",
       "      <td>0.587450</td>\n",
       "      <td>0.554613</td>\n",
       "      <td>0.229968</td>\n",
       "      <td>0.389487</td>\n",
       "      <td>0.225453</td>\n",
       "      <td>0.418001</td>\n",
       "      <td>0.263405</td>\n",
       "      <td>0.678585</td>\n",
       "      <td>0.466968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.443977</td>\n",
       "      <td>0.424711</td>\n",
       "      <td>0.526795</td>\n",
       "      <td>0.209773</td>\n",
       "      <td>0.316402</td>\n",
       "      <td>0.411231</td>\n",
       "      <td>0.398339</td>\n",
       "      <td>0.609873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60858</th>\n",
       "      <td>0.615468</td>\n",
       "      <td>0.055227</td>\n",
       "      <td>0.585383</td>\n",
       "      <td>0.960165</td>\n",
       "      <td>0.242121</td>\n",
       "      <td>0.101084</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.290930</td>\n",
       "      <td>0.663911</td>\n",
       "      <td>0.228861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371125</td>\n",
       "      <td>0.200191</td>\n",
       "      <td>0.042157</td>\n",
       "      <td>0.741037</td>\n",
       "      <td>0.931261</td>\n",
       "      <td>0.179951</td>\n",
       "      <td>0.192057</td>\n",
       "      <td>0.841595</td>\n",
       "      <td>0.680841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37041</th>\n",
       "      <td>0.247645</td>\n",
       "      <td>0.252975</td>\n",
       "      <td>0.472430</td>\n",
       "      <td>0.895060</td>\n",
       "      <td>0.266913</td>\n",
       "      <td>0.305077</td>\n",
       "      <td>0.096784</td>\n",
       "      <td>0.180987</td>\n",
       "      <td>0.401190</td>\n",
       "      <td>0.613721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535187</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>0.489294</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.929211</td>\n",
       "      <td>0.424409</td>\n",
       "      <td>0.402792</td>\n",
       "      <td>0.652253</td>\n",
       "      <td>0.441397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14422</th>\n",
       "      <td>0.473702</td>\n",
       "      <td>0.621110</td>\n",
       "      <td>0.485596</td>\n",
       "      <td>0.923814</td>\n",
       "      <td>0.483345</td>\n",
       "      <td>0.755059</td>\n",
       "      <td>0.317458</td>\n",
       "      <td>0.912570</td>\n",
       "      <td>0.201371</td>\n",
       "      <td>0.649508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.658438</td>\n",
       "      <td>0.229924</td>\n",
       "      <td>0.340058</td>\n",
       "      <td>0.443024</td>\n",
       "      <td>0.952947</td>\n",
       "      <td>0.645014</td>\n",
       "      <td>0.620591</td>\n",
       "      <td>0.984150</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32729</th>\n",
       "      <td>0.642131</td>\n",
       "      <td>0.464782</td>\n",
       "      <td>0.341651</td>\n",
       "      <td>0.851780</td>\n",
       "      <td>0.149229</td>\n",
       "      <td>0.506105</td>\n",
       "      <td>0.400209</td>\n",
       "      <td>0.627115</td>\n",
       "      <td>0.264778</td>\n",
       "      <td>0.378553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485076</td>\n",
       "      <td>0.459087</td>\n",
       "      <td>0.064581</td>\n",
       "      <td>0.940547</td>\n",
       "      <td>0.622032</td>\n",
       "      <td>0.590624</td>\n",
       "      <td>0.640839</td>\n",
       "      <td>0.646938</td>\n",
       "      <td>0.244663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85191</th>\n",
       "      <td>0.956559</td>\n",
       "      <td>0.892117</td>\n",
       "      <td>0.144696</td>\n",
       "      <td>0.442286</td>\n",
       "      <td>0.080573</td>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.976715</td>\n",
       "      <td>0.055495</td>\n",
       "      <td>0.697097</td>\n",
       "      <td>0.518840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116615</td>\n",
       "      <td>0.952311</td>\n",
       "      <td>0.136662</td>\n",
       "      <td>0.876693</td>\n",
       "      <td>0.363951</td>\n",
       "      <td>0.100704</td>\n",
       "      <td>0.255280</td>\n",
       "      <td>0.687254</td>\n",
       "      <td>0.723152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78786</th>\n",
       "      <td>0.796721</td>\n",
       "      <td>0.827191</td>\n",
       "      <td>0.562501</td>\n",
       "      <td>0.400240</td>\n",
       "      <td>0.045727</td>\n",
       "      <td>0.025473</td>\n",
       "      <td>0.876220</td>\n",
       "      <td>0.102423</td>\n",
       "      <td>0.989937</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157466</td>\n",
       "      <td>0.904173</td>\n",
       "      <td>0.129123</td>\n",
       "      <td>0.789514</td>\n",
       "      <td>0.611513</td>\n",
       "      <td>0.024785</td>\n",
       "      <td>0.015996</td>\n",
       "      <td>0.306764</td>\n",
       "      <td>0.967408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24399</th>\n",
       "      <td>0.260473</td>\n",
       "      <td>0.160570</td>\n",
       "      <td>0.925190</td>\n",
       "      <td>0.711899</td>\n",
       "      <td>0.184976</td>\n",
       "      <td>0.191392</td>\n",
       "      <td>0.122635</td>\n",
       "      <td>0.031412</td>\n",
       "      <td>0.820566</td>\n",
       "      <td>0.207951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068264</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.017698</td>\n",
       "      <td>0.092299</td>\n",
       "      <td>0.696832</td>\n",
       "      <td>0.151081</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.839673</td>\n",
       "      <td>0.922540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77574</th>\n",
       "      <td>0.342186</td>\n",
       "      <td>0.283447</td>\n",
       "      <td>0.751640</td>\n",
       "      <td>0.159726</td>\n",
       "      <td>0.388537</td>\n",
       "      <td>0.270639</td>\n",
       "      <td>0.190381</td>\n",
       "      <td>0.508265</td>\n",
       "      <td>0.890597</td>\n",
       "      <td>0.155405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258841</td>\n",
       "      <td>0.241161</td>\n",
       "      <td>0.398368</td>\n",
       "      <td>0.840574</td>\n",
       "      <td>0.364830</td>\n",
       "      <td>0.045031</td>\n",
       "      <td>0.041518</td>\n",
       "      <td>0.273403</td>\n",
       "      <td>0.946607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "48473  0.443313  0.202047  0.433925  0.900588  0.676421  0.994305  0.015536   \n",
       "34953  0.413209  0.587450  0.554613  0.229968  0.389487  0.225453  0.418001   \n",
       "60858  0.615468  0.055227  0.585383  0.960165  0.242121  0.101084  0.509524   \n",
       "37041  0.247645  0.252975  0.472430  0.895060  0.266913  0.305077  0.096784   \n",
       "14422  0.473702  0.621110  0.485596  0.923814  0.483345  0.755059  0.317458   \n",
       "32729  0.642131  0.464782  0.341651  0.851780  0.149229  0.506105  0.400209   \n",
       "85191  0.956559  0.892117  0.144696  0.442286  0.080573  0.026581  0.976715   \n",
       "78786  0.796721  0.827191  0.562501  0.400240  0.045727  0.025473  0.876220   \n",
       "24399  0.260473  0.160570  0.925190  0.711899  0.184976  0.191392  0.122635   \n",
       "77574  0.342186  0.283447  0.751640  0.159726  0.388537  0.270639  0.190381   \n",
       "\n",
       "       feature8  feature9  feature10   ...    feature13  feature14  feature15  \\\n",
       "48473  0.922447  0.145433   0.815773   ...     0.917480   0.028845   0.625409   \n",
       "34953  0.263405  0.678585   0.466968   ...     0.499500   0.443977   0.424711   \n",
       "60858  0.290930  0.663911   0.228861   ...     0.371125   0.200191   0.042157   \n",
       "37041  0.180987  0.401190   0.613721   ...     0.535187   0.152590   0.489294   \n",
       "14422  0.912570  0.201371   0.649508   ...     0.658438   0.229924   0.340058   \n",
       "32729  0.627115  0.264778   0.378553   ...     0.485076   0.459087   0.064581   \n",
       "85191  0.055495  0.697097   0.518840   ...     0.116615   0.952311   0.136662   \n",
       "78786  0.102423  0.989937   0.060900   ...     0.157466   0.904173   0.129123   \n",
       "24399  0.031412  0.820566   0.207951   ...     0.068264   0.033113   0.017698   \n",
       "77574  0.508265  0.890597   0.155405   ...     0.258841   0.241161   0.398368   \n",
       "\n",
       "       feature16  feature17  feature18  feature19  feature20  feature21  \\\n",
       "48473   0.391161   0.755612   0.842537   0.776601   0.962662   0.177580   \n",
       "34953   0.526795   0.209773   0.316402   0.411231   0.398339   0.609873   \n",
       "60858   0.741037   0.931261   0.179951   0.192057   0.841595   0.680841   \n",
       "37041   0.008503   0.929211   0.424409   0.402792   0.652253   0.441397   \n",
       "14422   0.443024   0.952947   0.645014   0.620591   0.984150   0.035088   \n",
       "32729   0.940547   0.622032   0.590624   0.640839   0.646938   0.244663   \n",
       "85191   0.876693   0.363951   0.100704   0.255280   0.687254   0.723152   \n",
       "78786   0.789514   0.611513   0.024785   0.015996   0.306764   0.967408   \n",
       "24399   0.092299   0.696832   0.151081   0.095900   0.839673   0.922540   \n",
       "77574   0.840574   0.364830   0.045031   0.041518   0.273403   0.946607   \n",
       "\n",
       "       target  \n",
       "48473       0  \n",
       "34953       1  \n",
       "60858       1  \n",
       "37041       0  \n",
       "14422       0  \n",
       "32729       0  \n",
       "85191       1  \n",
       "78786       1  \n",
       "24399       1  \n",
       "77574       0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの並びをシャッフル\n",
    "calcData = inputCSV.reindex(np.random.permutation(inputCSV.index))\n",
    "calcData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 入力データと教師データに分割\n",
    "pd_y = calcData['target']\n",
    "pd_x = calcData.drop(['target'], axis=1)\n",
    "\n",
    "# numpy配列にデータフレームから変換\n",
    "x_array = pd_x.as_matrix()\n",
    "y_array = pd_y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96320"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohno/.pyenv/versions/3.5.1/envs/devkeras/lib/python3.5/site-packages/numpy/lib/shape_base.py:422: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  sub_arys.append(_nx.swapaxes(sary[st:end], axis, 0))\n"
     ]
    }
   ],
   "source": [
    "# 正規化し,float32の型に変換\n",
    "x_scale = scale(x_array).astype(np.float32)\n",
    "\n",
    "# サンプルの一部をテストデータ用に分割\n",
    "N = pd_x.shape[0] * 0.1\n",
    "X_train, X_test = np.split(x_scale, [N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師データのラベリング\n",
    "変換が必要な点に注意."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "nb_classes = 2 # 2値分類\n",
    "nb_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohno/.pyenv/versions/3.5.1/envs/devkeras/lib/python3.5/site-packages/numpy/lib/shape_base.py:422: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  sub_arys.append(_nx.swapaxes(sary[st:end], axis, 0))\n"
     ]
    }
   ],
   "source": [
    "y_scale = y_array.astype(np.int32)\n",
    "Y_train, Y_test = np.split(y_scale, [N])\n",
    "\n",
    "# 離散の分類問題の場合、ベクトルのラベルをto_categoricalで変換しておく必要がある.\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learningのモデル定義\n",
    "\n",
    "以下では暫定的に多層ニューラルネットを用いる.\n",
    "\n",
    "(ここが基本的に変更する場所である.)\n",
    "\n",
    "なお、活性化関数にReLUを用いる.\n",
    "\n",
    "また途中に過学習を防ぎ汎化性能を上げる手法であるdropoutが仕込んである."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(80, input_shape=(21,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(80))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルは一度コンパイルする形式を取るタイプ\n",
    "\n",
    "サマリーが出てちょっと見やすい."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_5 (Dense)                  (None, 80)            1760        dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 80)            0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 80)            0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 100)           8100        dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 100)           0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 100)           0           activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 80)            8080        dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 80)            0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 80)            0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 2)             162         dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 2)             0           dense_8[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 18102\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_cb = keras.callbacks.EarlyStopping(monitor='val_acc', patience=50, verbose=1, mode='auto')\n",
    "log_filepath = os.curdir\n",
    "#tb_cb = keras.callbacks.TensorBoard(log_dir=log_filepath, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習と結果\n",
    "\n",
    "fitで学習し,evaluateでtestデータから汎化性能をチェックする."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9632 samples, validate on 86688 samples\n",
      "Epoch 1/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.7233 - acc: 0.5022 - val_loss: 0.6938 - val_acc: 0.5113\n",
      "Epoch 2/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.7049 - acc: 0.5128 - val_loss: 0.6933 - val_acc: 0.5125\n",
      "Epoch 3/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.7030 - acc: 0.5103 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 4/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.7002 - acc: 0.5077 - val_loss: 0.6929 - val_acc: 0.5145\n",
      "Epoch 5/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6987 - acc: 0.5096 - val_loss: 0.6928 - val_acc: 0.5135\n",
      "Epoch 6/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6985 - acc: 0.4984 - val_loss: 0.6927 - val_acc: 0.5150\n",
      "Epoch 7/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6962 - acc: 0.5119 - val_loss: 0.6927 - val_acc: 0.5150\n",
      "Epoch 8/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6959 - acc: 0.5059 - val_loss: 0.6927 - val_acc: 0.5156\n",
      "Epoch 9/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6957 - acc: 0.5129 - val_loss: 0.6927 - val_acc: 0.5150\n",
      "Epoch 10/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6949 - acc: 0.5116 - val_loss: 0.6927 - val_acc: 0.5143\n",
      "Epoch 11/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6933 - acc: 0.5240 - val_loss: 0.6927 - val_acc: 0.5146\n",
      "Epoch 12/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6946 - acc: 0.5093 - val_loss: 0.6926 - val_acc: 0.5149\n",
      "Epoch 13/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6947 - acc: 0.5093 - val_loss: 0.6926 - val_acc: 0.5148\n",
      "Epoch 14/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6927 - acc: 0.5209 - val_loss: 0.6927 - val_acc: 0.5144\n",
      "Epoch 15/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6923 - acc: 0.5211 - val_loss: 0.6926 - val_acc: 0.5148\n",
      "Epoch 16/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6931 - acc: 0.5175 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 17/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6928 - acc: 0.5214 - val_loss: 0.6926 - val_acc: 0.5160\n",
      "Epoch 18/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6925 - acc: 0.5172 - val_loss: 0.6926 - val_acc: 0.5148\n",
      "Epoch 19/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6931 - acc: 0.5120 - val_loss: 0.6926 - val_acc: 0.5152\n",
      "Epoch 20/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6921 - acc: 0.5189 - val_loss: 0.6926 - val_acc: 0.5158\n",
      "Epoch 21/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6920 - acc: 0.5222 - val_loss: 0.6926 - val_acc: 0.5157\n",
      "Epoch 22/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6924 - acc: 0.5236 - val_loss: 0.6926 - val_acc: 0.5154\n",
      "Epoch 23/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6918 - acc: 0.5224 - val_loss: 0.6926 - val_acc: 0.5152\n",
      "Epoch 24/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6920 - acc: 0.5232 - val_loss: 0.6926 - val_acc: 0.5148\n",
      "Epoch 25/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6923 - acc: 0.5174 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 26/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6917 - acc: 0.5250 - val_loss: 0.6926 - val_acc: 0.5156\n",
      "Epoch 27/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6923 - acc: 0.5216 - val_loss: 0.6925 - val_acc: 0.5163\n",
      "Epoch 28/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6908 - acc: 0.5248 - val_loss: 0.6925 - val_acc: 0.5155\n",
      "Epoch 29/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6916 - acc: 0.5210 - val_loss: 0.6926 - val_acc: 0.5157\n",
      "Epoch 30/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6917 - acc: 0.5194 - val_loss: 0.6925 - val_acc: 0.5148\n",
      "Epoch 31/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6912 - acc: 0.5254 - val_loss: 0.6925 - val_acc: 0.5155\n",
      "Epoch 32/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6915 - acc: 0.5164 - val_loss: 0.6925 - val_acc: 0.5170\n",
      "Epoch 33/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6915 - acc: 0.5279 - val_loss: 0.6925 - val_acc: 0.5163\n",
      "Epoch 34/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6903 - acc: 0.5343 - val_loss: 0.6925 - val_acc: 0.5161\n",
      "Epoch 35/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6913 - acc: 0.5179 - val_loss: 0.6925 - val_acc: 0.5161\n",
      "Epoch 36/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6904 - acc: 0.5300 - val_loss: 0.6925 - val_acc: 0.5161\n",
      "Epoch 37/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6908 - acc: 0.5264 - val_loss: 0.6925 - val_acc: 0.5162\n",
      "Epoch 38/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6904 - acc: 0.5273 - val_loss: 0.6925 - val_acc: 0.5163\n",
      "Epoch 39/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6906 - acc: 0.5309 - val_loss: 0.6925 - val_acc: 0.5162\n",
      "Epoch 40/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6912 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5162\n",
      "Epoch 41/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6904 - acc: 0.5323 - val_loss: 0.6925 - val_acc: 0.5163\n",
      "Epoch 42/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6905 - acc: 0.5269 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "Epoch 43/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6898 - acc: 0.5288 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "Epoch 44/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6902 - acc: 0.5214 - val_loss: 0.6925 - val_acc: 0.5164\n",
      "Epoch 45/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6907 - acc: 0.5235 - val_loss: 0.6925 - val_acc: 0.5166\n",
      "Epoch 46/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6908 - acc: 0.5250 - val_loss: 0.6925 - val_acc: 0.5169\n",
      "Epoch 47/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6892 - acc: 0.5294 - val_loss: 0.6926 - val_acc: 0.5160\n",
      "Epoch 48/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6898 - acc: 0.5334 - val_loss: 0.6925 - val_acc: 0.5159\n",
      "Epoch 49/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6890 - acc: 0.5337 - val_loss: 0.6925 - val_acc: 0.5166\n",
      "Epoch 50/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6902 - acc: 0.5305 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "Epoch 51/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6890 - acc: 0.5361 - val_loss: 0.6926 - val_acc: 0.5169\n",
      "Epoch 52/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6901 - acc: 0.5252 - val_loss: 0.6925 - val_acc: 0.5166\n",
      "Epoch 53/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6883 - acc: 0.5362 - val_loss: 0.6926 - val_acc: 0.5164\n",
      "Epoch 54/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6890 - acc: 0.5307 - val_loss: 0.6926 - val_acc: 0.5166\n",
      "Epoch 55/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6886 - acc: 0.5368 - val_loss: 0.6926 - val_acc: 0.5157\n",
      "Epoch 56/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6899 - acc: 0.5281 - val_loss: 0.6926 - val_acc: 0.5164\n",
      "Epoch 57/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6896 - acc: 0.5325 - val_loss: 0.6926 - val_acc: 0.5163\n",
      "Epoch 58/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6885 - acc: 0.5371 - val_loss: 0.6926 - val_acc: 0.5160\n",
      "Epoch 59/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6898 - acc: 0.5255 - val_loss: 0.6926 - val_acc: 0.5157\n",
      "Epoch 60/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6882 - acc: 0.5352 - val_loss: 0.6926 - val_acc: 0.5161\n",
      "Epoch 61/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6900 - acc: 0.5354 - val_loss: 0.6926 - val_acc: 0.5151\n",
      "Epoch 62/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6892 - acc: 0.5319 - val_loss: 0.6927 - val_acc: 0.5159\n",
      "Epoch 63/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6892 - acc: 0.5307 - val_loss: 0.6927 - val_acc: 0.5158\n",
      "Epoch 64/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6883 - acc: 0.5353 - val_loss: 0.6927 - val_acc: 0.5156\n",
      "Epoch 65/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6890 - acc: 0.5323 - val_loss: 0.6927 - val_acc: 0.5156\n",
      "Epoch 66/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6879 - acc: 0.5395 - val_loss: 0.6927 - val_acc: 0.5154\n",
      "Epoch 67/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6893 - acc: 0.5324 - val_loss: 0.6927 - val_acc: 0.5149\n",
      "Epoch 68/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6885 - acc: 0.5317 - val_loss: 0.6928 - val_acc: 0.5146\n",
      "Epoch 69/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6889 - acc: 0.5294 - val_loss: 0.6927 - val_acc: 0.5136\n",
      "Epoch 70/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6879 - acc: 0.5366 - val_loss: 0.6928 - val_acc: 0.5137\n",
      "Epoch 71/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6875 - acc: 0.5383 - val_loss: 0.6928 - val_acc: 0.5150\n",
      "Epoch 72/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6884 - acc: 0.5400 - val_loss: 0.6927 - val_acc: 0.5149\n",
      "Epoch 73/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6879 - acc: 0.5415 - val_loss: 0.6927 - val_acc: 0.5156\n",
      "Epoch 74/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6880 - acc: 0.5380 - val_loss: 0.6927 - val_acc: 0.5148\n",
      "Epoch 75/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6877 - acc: 0.5360 - val_loss: 0.6927 - val_acc: 0.5147\n",
      "Epoch 76/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6881 - acc: 0.5417 - val_loss: 0.6927 - val_acc: 0.5156\n",
      "Epoch 77/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6874 - acc: 0.5355 - val_loss: 0.6928 - val_acc: 0.5151\n",
      "Epoch 78/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6874 - acc: 0.5402 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 79/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6881 - acc: 0.5417 - val_loss: 0.6928 - val_acc: 0.5147\n",
      "Epoch 80/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6878 - acc: 0.5449 - val_loss: 0.6928 - val_acc: 0.5144\n",
      "Epoch 81/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6878 - acc: 0.5387 - val_loss: 0.6928 - val_acc: 0.5139\n",
      "Epoch 82/200\n",
      "9632/9632 [==============================] - 0s - loss: 0.6878 - acc: 0.5397 - val_loss: 0.6929 - val_acc: 0.5146\n",
      "Epoch 83/200\n",
      "8000/9632 [=======================>......] - ETA: 0s - loss: 0.6870 - acc: 0.5431Epoch 00082: early stopping\n",
      "9632/9632 [==============================] - 0s - loss: 0.6871 - acc: 0.5459 - val_loss: 0.6929 - val_acc: 0.5142\n",
      "Test score: 0.692885888925\n",
      "Test accuracy: 0.514211886305\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test), callbacks=[es_cb])#, tb_cb])\n",
    "#keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33032</td>\n",
       "      <td>0.919136</td>\n",
       "      <td>0.402220</td>\n",
       "      <td>0.335447</td>\n",
       "      <td>0.408911</td>\n",
       "      <td>0.909374</td>\n",
       "      <td>0.875515</td>\n",
       "      <td>0.793247</td>\n",
       "      <td>0.735592</td>\n",
       "      <td>0.290002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823239</td>\n",
       "      <td>0.859311</td>\n",
       "      <td>0.586265</td>\n",
       "      <td>0.509609</td>\n",
       "      <td>0.768567</td>\n",
       "      <td>0.024489</td>\n",
       "      <td>0.939576</td>\n",
       "      <td>0.596841</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.188507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20838</td>\n",
       "      <td>0.072862</td>\n",
       "      <td>0.085398</td>\n",
       "      <td>0.691351</td>\n",
       "      <td>0.917969</td>\n",
       "      <td>0.184682</td>\n",
       "      <td>0.499284</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.115818</td>\n",
       "      <td>0.781117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135286</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>0.128603</td>\n",
       "      <td>0.094148</td>\n",
       "      <td>0.858657</td>\n",
       "      <td>0.567579</td>\n",
       "      <td>0.605002</td>\n",
       "      <td>0.882755</td>\n",
       "      <td>0.960829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17585</td>\n",
       "      <td>0.268903</td>\n",
       "      <td>0.470812</td>\n",
       "      <td>0.178869</td>\n",
       "      <td>0.687795</td>\n",
       "      <td>0.622515</td>\n",
       "      <td>0.605008</td>\n",
       "      <td>0.832596</td>\n",
       "      <td>0.527291</td>\n",
       "      <td>0.202580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615814</td>\n",
       "      <td>0.611614</td>\n",
       "      <td>0.552004</td>\n",
       "      <td>0.631388</td>\n",
       "      <td>0.430585</td>\n",
       "      <td>0.447971</td>\n",
       "      <td>0.435045</td>\n",
       "      <td>0.320062</td>\n",
       "      <td>0.702936</td>\n",
       "      <td>0.266742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4231</td>\n",
       "      <td>0.291499</td>\n",
       "      <td>0.100183</td>\n",
       "      <td>0.108386</td>\n",
       "      <td>0.832005</td>\n",
       "      <td>0.202355</td>\n",
       "      <td>0.478545</td>\n",
       "      <td>0.166413</td>\n",
       "      <td>0.368036</td>\n",
       "      <td>0.209103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329354</td>\n",
       "      <td>0.374242</td>\n",
       "      <td>0.123975</td>\n",
       "      <td>0.482507</td>\n",
       "      <td>0.780585</td>\n",
       "      <td>0.432857</td>\n",
       "      <td>0.383226</td>\n",
       "      <td>0.509814</td>\n",
       "      <td>0.770422</td>\n",
       "      <td>0.341161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33357</td>\n",
       "      <td>0.151855</td>\n",
       "      <td>0.067270</td>\n",
       "      <td>0.236928</td>\n",
       "      <td>0.600259</td>\n",
       "      <td>0.892697</td>\n",
       "      <td>0.611378</td>\n",
       "      <td>0.081095</td>\n",
       "      <td>0.617280</td>\n",
       "      <td>0.336961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897775</td>\n",
       "      <td>0.390264</td>\n",
       "      <td>0.040880</td>\n",
       "      <td>0.788739</td>\n",
       "      <td>0.562621</td>\n",
       "      <td>0.816385</td>\n",
       "      <td>0.459390</td>\n",
       "      <td>0.446632</td>\n",
       "      <td>0.293370</td>\n",
       "      <td>0.215816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17425</td>\n",
       "      <td>0.243205</td>\n",
       "      <td>0.620738</td>\n",
       "      <td>0.884520</td>\n",
       "      <td>0.993783</td>\n",
       "      <td>0.148403</td>\n",
       "      <td>0.636627</td>\n",
       "      <td>0.358717</td>\n",
       "      <td>0.545112</td>\n",
       "      <td>0.583144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.245494</td>\n",
       "      <td>0.488298</td>\n",
       "      <td>0.269465</td>\n",
       "      <td>0.219151</td>\n",
       "      <td>0.960993</td>\n",
       "      <td>0.751257</td>\n",
       "      <td>0.663983</td>\n",
       "      <td>0.847568</td>\n",
       "      <td>0.606422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27310</td>\n",
       "      <td>0.433634</td>\n",
       "      <td>0.129248</td>\n",
       "      <td>0.457350</td>\n",
       "      <td>0.500149</td>\n",
       "      <td>0.988647</td>\n",
       "      <td>0.980214</td>\n",
       "      <td>0.062949</td>\n",
       "      <td>0.975227</td>\n",
       "      <td>0.076214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866592</td>\n",
       "      <td>0.828941</td>\n",
       "      <td>0.026036</td>\n",
       "      <td>0.957695</td>\n",
       "      <td>0.726683</td>\n",
       "      <td>0.570812</td>\n",
       "      <td>0.683132</td>\n",
       "      <td>0.793599</td>\n",
       "      <td>0.259133</td>\n",
       "      <td>0.073043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16522</td>\n",
       "      <td>0.478147</td>\n",
       "      <td>0.239203</td>\n",
       "      <td>0.472099</td>\n",
       "      <td>0.705242</td>\n",
       "      <td>0.636406</td>\n",
       "      <td>0.272477</td>\n",
       "      <td>0.076986</td>\n",
       "      <td>0.626842</td>\n",
       "      <td>0.465995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812237</td>\n",
       "      <td>0.499135</td>\n",
       "      <td>0.272245</td>\n",
       "      <td>0.925725</td>\n",
       "      <td>0.238305</td>\n",
       "      <td>0.875119</td>\n",
       "      <td>0.435045</td>\n",
       "      <td>0.583181</td>\n",
       "      <td>0.674225</td>\n",
       "      <td>0.378057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25340</td>\n",
       "      <td>0.690443</td>\n",
       "      <td>0.733230</td>\n",
       "      <td>0.669029</td>\n",
       "      <td>0.149143</td>\n",
       "      <td>0.166520</td>\n",
       "      <td>0.086494</td>\n",
       "      <td>0.542064</td>\n",
       "      <td>0.171097</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058273</td>\n",
       "      <td>0.133734</td>\n",
       "      <td>0.752680</td>\n",
       "      <td>0.103004</td>\n",
       "      <td>0.615365</td>\n",
       "      <td>0.040236</td>\n",
       "      <td>0.184330</td>\n",
       "      <td>0.163866</td>\n",
       "      <td>0.090175</td>\n",
       "      <td>0.509583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4840</td>\n",
       "      <td>0.651826</td>\n",
       "      <td>0.621780</td>\n",
       "      <td>0.636015</td>\n",
       "      <td>0.211715</td>\n",
       "      <td>0.816277</td>\n",
       "      <td>0.476272</td>\n",
       "      <td>0.507397</td>\n",
       "      <td>0.680482</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878738</td>\n",
       "      <td>0.595791</td>\n",
       "      <td>0.675861</td>\n",
       "      <td>0.753440</td>\n",
       "      <td>0.341442</td>\n",
       "      <td>0.322559</td>\n",
       "      <td>0.384265</td>\n",
       "      <td>0.687706</td>\n",
       "      <td>0.183704</td>\n",
       "      <td>0.338938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    t_id  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0  33032  0.919136  0.402220  0.335447  0.408911  0.909374  0.875515   \n",
       "1  20838  0.072862  0.085398  0.691351  0.917969  0.184682  0.499284   \n",
       "2  17585  0.268903  0.470812  0.178869  0.687795  0.622515  0.605008   \n",
       "3   4231  0.291499  0.100183  0.108386  0.832005  0.202355  0.478545   \n",
       "4  33357  0.151855  0.067270  0.236928  0.600259  0.892697  0.611378   \n",
       "5  17425  0.243205  0.620738  0.884520  0.993783  0.148403  0.636627   \n",
       "6  27310  0.433634  0.129248  0.457350  0.500149  0.988647  0.980214   \n",
       "7  16522  0.478147  0.239203  0.472099  0.705242  0.636406  0.272477   \n",
       "8  25340  0.690443  0.733230  0.669029  0.149143  0.166520  0.086494   \n",
       "9   4840  0.651826  0.621780  0.636015  0.211715  0.816277  0.476272   \n",
       "\n",
       "   feature7  feature8  feature9    ...      feature12  feature13  feature14  \\\n",
       "0  0.793247  0.735592  0.290002    ...       0.823239   0.859311   0.586265   \n",
       "1  0.025199  0.115818  0.781117    ...       0.135286   0.253641   0.019908   \n",
       "2  0.832596  0.527291  0.202580    ...       0.615814   0.611614   0.552004   \n",
       "3  0.166413  0.368036  0.209103    ...       0.329354   0.374242   0.123975   \n",
       "4  0.081095  0.617280  0.336961    ...       0.897775   0.390264   0.040880   \n",
       "5  0.358717  0.545112  0.583144    ...       0.025126   0.245494   0.488298   \n",
       "6  0.062949  0.975227  0.076214    ...       0.866592   0.828941   0.026036   \n",
       "7  0.076986  0.626842  0.465995    ...       0.812237   0.499135   0.272245   \n",
       "8  0.542064  0.171097  0.594200    ...       0.058273   0.133734   0.752680   \n",
       "9  0.507397  0.680482  0.558007    ...       0.878738   0.595791   0.675861   \n",
       "\n",
       "   feature15  feature16  feature17  feature18  feature19  feature20  feature21  \n",
       "0   0.509609   0.768567   0.024489   0.939576   0.596841   0.454167   0.188507  \n",
       "1   0.128603   0.094148   0.858657   0.567579   0.605002   0.882755   0.960829  \n",
       "2   0.631388   0.430585   0.447971   0.435045   0.320062   0.702936   0.266742  \n",
       "3   0.482507   0.780585   0.432857   0.383226   0.509814   0.770422   0.341161  \n",
       "4   0.788739   0.562621   0.816385   0.459390   0.446632   0.293370   0.215816  \n",
       "5   0.269465   0.219151   0.960993   0.751257   0.663983   0.847568   0.606422  \n",
       "6   0.957695   0.726683   0.570812   0.683132   0.793599   0.259133   0.073043  \n",
       "7   0.925725   0.238305   0.875119   0.435045   0.583181   0.674225   0.378057  \n",
       "8   0.103004   0.615365   0.040236   0.184330   0.163866   0.090175   0.509583  \n",
       "9   0.753440   0.341442   0.322559   0.384265   0.687706   0.183704   0.338938  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.read_csv(\"numerai_tournament_data.csv\")\n",
    "pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature12</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.919136</td>\n",
       "      <td>0.402220</td>\n",
       "      <td>0.335447</td>\n",
       "      <td>0.408911</td>\n",
       "      <td>0.909374</td>\n",
       "      <td>0.875515</td>\n",
       "      <td>0.793247</td>\n",
       "      <td>0.735592</td>\n",
       "      <td>0.290002</td>\n",
       "      <td>0.984229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823239</td>\n",
       "      <td>0.859311</td>\n",
       "      <td>0.586265</td>\n",
       "      <td>0.509609</td>\n",
       "      <td>0.768567</td>\n",
       "      <td>0.024489</td>\n",
       "      <td>0.939576</td>\n",
       "      <td>0.596841</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.188507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072862</td>\n",
       "      <td>0.085398</td>\n",
       "      <td>0.691351</td>\n",
       "      <td>0.917969</td>\n",
       "      <td>0.184682</td>\n",
       "      <td>0.499284</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.115818</td>\n",
       "      <td>0.781117</td>\n",
       "      <td>0.360633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135286</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>0.128603</td>\n",
       "      <td>0.094148</td>\n",
       "      <td>0.858657</td>\n",
       "      <td>0.567579</td>\n",
       "      <td>0.605002</td>\n",
       "      <td>0.882755</td>\n",
       "      <td>0.960829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.268903</td>\n",
       "      <td>0.470812</td>\n",
       "      <td>0.178869</td>\n",
       "      <td>0.687795</td>\n",
       "      <td>0.622515</td>\n",
       "      <td>0.605008</td>\n",
       "      <td>0.832596</td>\n",
       "      <td>0.527291</td>\n",
       "      <td>0.202580</td>\n",
       "      <td>0.359053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615814</td>\n",
       "      <td>0.611614</td>\n",
       "      <td>0.552004</td>\n",
       "      <td>0.631388</td>\n",
       "      <td>0.430585</td>\n",
       "      <td>0.447971</td>\n",
       "      <td>0.435045</td>\n",
       "      <td>0.320062</td>\n",
       "      <td>0.702936</td>\n",
       "      <td>0.266742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.919136  0.402220  0.335447  0.408911  0.909374  0.875515  0.793247   \n",
       "1  0.072862  0.085398  0.691351  0.917969  0.184682  0.499284  0.025199   \n",
       "2  0.268903  0.470812  0.178869  0.687795  0.622515  0.605008  0.832596   \n",
       "\n",
       "   feature8  feature9  feature10    ...      feature12  feature13  feature14  \\\n",
       "0  0.735592  0.290002   0.984229    ...       0.823239   0.859311   0.586265   \n",
       "1  0.115818  0.781117   0.360633    ...       0.135286   0.253641   0.019908   \n",
       "2  0.527291  0.202580   0.359053    ...       0.615814   0.611614   0.552004   \n",
       "\n",
       "   feature15  feature16  feature17  feature18  feature19  feature20  feature21  \n",
       "0   0.509609   0.768567   0.024489   0.939576   0.596841   0.454167   0.188507  \n",
       "1   0.128603   0.094148   0.858657   0.567579   0.605002   0.882755   0.960829  \n",
       "2   0.631388   0.430585   0.447971   0.435045   0.320062   0.702936   0.266742  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_x = pred.drop([\"t_id\"],axis=1)\n",
    "pred_x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_pred = pred_x.as_matrix()\n",
    "\n",
    "ans = model.predict(np_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.518254\n",
       "1    0.502082\n",
       "2    0.519519\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_ans = pd.DataFrame(ans)\n",
    "pd_ans[1].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_csv = pd.concat([pred[\"t_id\"],pd_ans[1]], axis=1)\n",
    "pred_csv.columns = [\"t_id\", \"probability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33032</td>\n",
       "      <td>0.518254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20838</td>\n",
       "      <td>0.502082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17585</td>\n",
       "      <td>0.519519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4231</td>\n",
       "      <td>0.507580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33357</td>\n",
       "      <td>0.509699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17425</td>\n",
       "      <td>0.513522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27310</td>\n",
       "      <td>0.525883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16522</td>\n",
       "      <td>0.506156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25340</td>\n",
       "      <td>0.506088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4840</td>\n",
       "      <td>0.526192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    t_id  probability\n",
       "0  33032     0.518254\n",
       "1  20838     0.502082\n",
       "2  17585     0.519519\n",
       "3   4231     0.507580\n",
       "4  33357     0.509699\n",
       "5  17425     0.513522\n",
       "6  27310     0.525883\n",
       "7  16522     0.506156\n",
       "8  25340     0.506088\n",
       "9   4840     0.526192"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_csv.to_csv(\"pred_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
